import pandas as pd
import matplotlib.pyplot as plt

# -------------------
# Load data
# -------------------
df = pd.read_csv("your_file.csv")

# -------------------
# Identify columns
# -------------------
meta_cols = df.columns[:8].tolist()          # first 8 metadata columns
price_cols = df.columns[8:].tolist()         # from Unnamed:8 onwards (prices)

# -------------------
# Normalize to returns relative to t0 (add date price)
# -------------------
df_perf = df.copy()
P0 = df[price_cols[0]]   # price at add date (t0)

for c in price_cols:
    df_perf[c] = (df[c] / P0) - 1

# -------------------
# Fixed horizon analysis (approx 21 trading days per month)
# -------------------
months_map = {
    "1m": 21,
    "2m": 42,
    "3m": 63,
    "4m": 84,
    "5m": 105,
    "6m": 126
}

results = []

for label, day in months_map.items():
    # price_cols[0] is t0, so day offset = price_cols[day]
    if day < len(price_cols):  
        col_name = price_cols[day]  
        tmp = df_perf[[meta_cols[1], col_name]].copy()  # ticker + return
        tmp.rename(columns={col_name: "return"}, inplace=True)
        tmp["horizon"] = label
        results.append(tmp)

perf_long = pd.concat(results)

# Mean return per horizon
mean_perf = perf_long.groupby("horizon")["return"].mean()

# -------------------
# Full average performance curve
# -------------------
avg_curve = df_perf[price_cols].mean(axis=0)

# -------------------
# Plotting
# -------------------

# 1. Snapshot mean returns bar chart
plt.figure(figsize=(8,5))
mean_perf.plot(kind="bar")
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.ylabel("Average Return")
plt.title("Average Return at Fixed Horizons")
plt.show()

# 2. Distribution boxplots
plt.figure(figsize=(8,5))
perf_long.boxplot(by="horizon", column="return", grid=False)
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.ylabel("Return")
plt.title("Return Distribution by Horizon")
plt.suptitle("")  # remove default pandas title
plt.show()

# 3. Full performance curve (average cumulative return path)
plt.figure(figsize=(10,6))
avg_curve.plot()
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.ylabel("Average Return")
plt.xlabel("Days Since Add Date (t0)")
plt.title("Average Performance Curve Across All Names")
plt.show()


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------
# Load data
# -------------------
df = pd.read_csv("your_file.csv")

# -------------------
# Identify columns
# -------------------
bbg_col = df.columns[6]   # bbg_ticker
date_col = df.columns[7]  # add_date
price_cols = df.columns[8:].tolist()

# -------------------
# Normalize prices into returns
# -------------------
df_perf = df.copy()
P0 = df[price_cols[0]]  # price at t0 (add date)

for c in price_cols:
    df_perf[c] = (df[c] / P0) - 1

# -------------------
# Horizon mapping (approx 21 trading days ~ 1 month)
# -------------------
months_map = {
    "1m": 21,
    "2m": 42,
    "3m": 63,
    "4m": 84,
    "5m": 105,
    "6m": 126
}

results = []

for label, day in months_map.items():
    if day < len(price_cols):  # only if horizon exists
        col_name = price_cols[day]
        tmp = df_perf[[bbg_col, date_col, col_name]].copy()
        tmp.rename(columns={col_name: "return"}, inplace=True)
        tmp["horizon"] = label
        results.append(tmp)

perf_long = pd.concat(results)

# -------------------
# Average performance curve
# -------------------
avg_curve = df_perf[price_cols].mean(axis=0)
day_nums = list(range(len(price_cols)))  # 0,1,2,...
avg_curve.index = day_nums

plt.figure(figsize=(10,6))
avg_curve.plot()
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.ylabel("Average Return")
plt.xlabel("Days Since Add Date (t0)")
plt.title("Average Performance Curve Across All Names")
plt.show()

# -------------------
# Boxplot with labeled outliers
# -------------------
plt.figure(figsize=(10,6))
sns.boxplot(x="horizon", y="return", data=perf_long, showfliers=True)
sns.stripplot(x="horizon", y="return", data=perf_long, color="black", size=3, alpha=0.5)

# Label outliers (top & bottom 3 per horizon)
for horizon in perf_long["horizon"].unique():
    sub = perf_long[perf_long["horizon"] == horizon]
    top = sub.nlargest(3, "return")
    bottom = sub.nsmallest(3, "return")
    extremes = pd.concat([top, bottom])
    x_pos = list(perf_long["horizon"].unique()).tolist().index(horizon)
    for _, row in extremes.iterrows():
        plt.text(
            x=x_pos,
            y=row["return"],
            s=row[bbg_col],
            fontsize=8,
            ha="center",
            va="bottom" if row["return"] > 0 else "top"
        )

plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.ylabel("Return")
plt.title("Return Distribution by Horizon (with Outliers Labeled)")
plt.show()

# -------------------
# Stacked plot of all tickers' curves
# -------------------
plt.figure(figsize=(12,7))

for _, row in df_perf.iterrows():
    returns = row[price_cols].values
    returns = (returns / returns[0]) - 1  # normalize
    plt.plot(day_nums, returns, alpha=0.3, linewidth=1, color="gray")

# Overlay average curve
plt.plot(day_nums, avg_curve, color="red", linewidth=2, label="Average")

plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.ylabel("Return")
plt.xlabel("Days Since Add Date (t0)")
plt.title("All Tickers Performance Curves (Stacked)")
plt.legend()
plt.show()

Fund,Ticker/ISIN,Underlying Index,AuM
Xtrackers MSCI World UCITS ETF 1C,IE00BJ0KDQ92,MSCI World,€20.63B
Xtrackers S&P 500 UCITS ETF 4C,IE000Z9SJA06,S&P 500,€1.53B
Xtrackers Artificial Intelligence & Big Data UCITS ETF 1C,IE00BGV5VN51,Nasdaq Global AI & Big Data Index,€5.50B
Xtrackers II EUR Overnight Rate Swap UCITS ETF 1C,LU0290358497,EUR Overnight Rate Swap Index,€18.37M
Xtrackers DAX UCITS ETF 1C Core Direct,XDAX,DAX,€6.48B
Xtrackers Euro Stoxx 50 UCITS ETF 1D Core,LU0274211217,EURO STOXX 50,€9.94B
Xtrackers Artificial Intelligence & Big Data ETF (US),XAIX,Nasdaq Global AI & Big Data Index,Part of $21B suite
Xtrackers USD High Yield BB-B ex Financials ETF,HYLB,ICE BofA BB-B Non-Financials HY Index,$3.53B
Xtrackers US National Critical Technologies ETF,CRTC,Solactive Whitney US Critical Technologies Index,N/A
Xtrackers USA Net Zero Pathway Paris Aligned UCITS ETF 1C ESG,N/A,Solactive-ISS ESG USA Net Zero Pathway Index,N/A
Xtrackers Europe Net Zero Pathway Paris Aligned UCITS ETF 1C ESG,N/A,Solactive-ISS ESG Europe Net Zero Pathway Index,N/A
Xtrackers Japan Net Zero Pathway Paris Aligned UCITS ETF 1C ESG,N/A,Solactive-ISS ESG Japan Net Zero Pathway Index,N/A
Xtrackers Emerging Markets Net Zero Pathway Paris Aligned UCITS ETF 1C ESG,N/A,Solactive-ISS ESG EM Net Zero Pathway Index,N/A
Xtrackers MSCI China A ESG Screened Swap UCITS ETF 1C,LU2469465822,MSCI China A Inclusion Select ESG Screened Index,€5.13M
Xtrackers CSI300 Swap UCITS ETF 1C,LU0779800910,CSI 300 Swap Index,€2.05B
Xtrackers Nikkei 225 UCITS ETF 1D,XDJP,Nikkei 225,€1.60B
Xtrackers MSCI China UCITS ETF 1C,N/A,MSCI China,€1.88B


import pandas as pd
import matplotlib.pyplot as plt

# === Load your CSV ===
df = pd.read_csv("your_file.csv")

# Expected columns: ['side', 'quantity', 'bbg_ticker', 'add_date', price columns...]
price_cols = df.columns[4:]  # first 4 cols are meta, rest are Unnamed:8, Unnamed:9, ...

# Compute returns relative to trade date
returns = (df[price_cols].T / df[price_cols].iloc[:,0].values).T - 1
returns["bbg_ticker"] = df["bbg_ticker"]
returns["add_date"] = pd.to_datetime(df["add_date"])
returns["side"] = df["side"].str.lower()
returns["quantity"] = df["quantity"]

# Map side → +1 (long), -1 (short)
returns["weight"] = returns["quantity"] * returns["side"].map({"long": 1, "short": -1})

# === Group by trade date ===
portfolio_curves = {}

for trade_date, group in returns.groupby("add_date"):
    # Weighted average across tickers traded that day
    weighted = (group[price_cols].T * group["weight"].values).T
    portfolio_curve = weighted.sum(axis=0) / group["quantity"].sum()
    portfolio_curves[trade_date] = portfolio_curve

# Convert to DataFrame
portfolio_df = pd.DataFrame(portfolio_curves)

# Average performance across all trade dates
average_curve = portfolio_df.mean(axis=1)

# === Relabel X-axis as months (1m, 2m, …) ===
days = range(len(price_cols))
months = [f"{i}m" for i in range(1,7)]
month_positions = [21*i for i in range(1,7)]  # ~21 trading days = 1m

# === Plot portfolio curves ===
plt.figure(figsize=(12,7))

# Plot each trade date curve
for trade_date in portfolio_df.columns:
    plt.plot(days, portfolio_df[trade_date].values, alpha=0.4, label=str(trade_date.date()))

# Plot average curve
plt.plot(days, average_curve.values, color="black", linewidth=2, label="Average")

plt.axhline(0, color="black", linewidth=0.8)
plt.xticks(month_positions, months)
plt.xlabel("Time since trade date")
plt.ylabel("Performance vs. trade date (%)")
plt.title("Portfolio Performance by Trade Date")
plt.legend(loc="best", fontsize=8)
plt.grid(True)
plt.show()



---


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ---------- load ----------
df = pd.read_csv("your_file.csv")   # <-- change filename if needed

# ---------- find core columns (case-insensitive) ----------
cols_lower = {c.lower(): c for c in df.columns}

def find_col(names):
    for n in names:
        if n.lower() in cols_lower:
            return cols_lower[n.lower()]
    return None

side_col   = find_col(['side'])
qty_col    = find_col(['quantity','qty','size'])
ticker_col = find_col(['bbg_ticker','ticker','symbol'])
date_col   = find_col(['add_date','trade_date','date'])

if not all([side_col, qty_col, ticker_col, date_col]):
    missing = [n for n,c in [('side',side_col),('quantity',qty_col),('bbg_ticker',ticker_col),('add_date',date_col)] if c is None]
    raise ValueError(f"Could not find required columns in CSV: {missing}. Column names are case-insensitive; adapt names or rename columns.")

# ---------- identify price columns (everything else) ----------
meta_cols = {side_col, qty_col, ticker_col, date_col}
price_cols = [c for c in df.columns if c not in meta_cols]
if len(price_cols) == 0:
    raise ValueError("No price columns found. Price columns should be all columns after the meta columns.")

# ---------- prepare numeric price matrix ----------
prices = df[price_cols].apply(pd.to_numeric, errors='coerce').to_numpy(dtype=float)   # shape (n_rows, n_days)
n_days = prices.shape[1]
day_idx = np.arange(n_days)   # 0,1,2,...

# ---------- compute row-wise returns relative to t0 (first price column) ----------
P0 = prices[:, 0].astype(float)
valid = np.isfinite(P0) & (P0 != 0)
returns = np.full_like(prices, np.nan, dtype=float)
returns[valid, :] = prices[valid, :] / P0[valid, None] - 1.0

# ---------- meta dataframe ----------
meta = pd.DataFrame({
    'side_raw': df[side_col].astype(str),
    'quantity': pd.to_numeric(df[qty_col], errors='coerce').fillna(1.0),
    'ticker': df[ticker_col].astype(str),
    'trade_date': pd.to_datetime(df[date_col], errors='coerce')
})

# drop rows with invalid trade_date
meta = meta.reset_index(drop=False)  # keep original row index in column 'index'
meta.rename(columns={'index':'orig_index'}, inplace=True)

# ---------- map sides Buy/Sell to +1/-1 ----------
meta['side_norm'] = meta['side_raw'].str.strip().str.lower().map({'buy': 1, 'sell': -1})
# Any unmapped side -> raise warning and drop those rows
unmapped = meta[meta['side_norm'].isna()]
if not unmapped.empty:
    print("Warning: some 'side' values were not 'Buy' or 'Sell' (case-insensitive). These rows will be dropped:")
    print(unmapped[['orig_index','side_raw']].head())
    meta = meta[meta['side_norm'].notna()]

# build signed weights = quantity * sign
meta['signed_weight'] = meta['quantity'] * meta['side_norm']

# ---------- group by trade_date and compute portfolio curve per date ----------
portfolio_gross = {}   # gross-normalized (divide by sum abs weights present that day)
portfolio_net   = {}   # net-normalized (divide by sum signed weights) when possible

# Use the original row numbers to pick rows from returns matrix
for trade_date, group in meta.groupby('trade_date', sort=True):
    idxs = group['orig_index'].to_numpy(dtype=int)   # indices into returns matrix
    group_returns = returns[idxs, :]                 # shape (n_group, n_days)
    group_weights = group['signed_weight'].to_numpy(dtype=float)  # length n_group

    # skip groups with no valid returns (all NaN)
    if np.isnan(group_returns).all():
        continue

    # For each day compute weighted sum ignoring NaNs:
    # numerator_day = sum_i ( weight_i * return_{i,day} ) where return not NaN
    # denom_gross_day = sum_i ( |weight_i| ) where return not NaN
    mask_valid = ~np.isnan(group_returns)   # True where a given ticker has data that day
    weighted_vals = np.where(mask_valid, group_returns * group_weights[:, None], 0.0)
    numerator = weighted_vals.sum(axis=0)   # length n_days

    # per-day denom: sum of abs(weights) for tickers present that day
    denom_gross_per_day = (mask_valid * np.abs(group_weights)[:, None]).sum(axis=0)  # length n_days

    # build gross-normalized portfolio curve (per day)
    pg = np.full(n_days, np.nan)
    ok = denom_gross_per_day != 0
    pg[ok] = numerator[ok] / denom_gross_per_day[ok]

    # net-normalized (divide by sum of signed weights) if sum != 0
    denom_net = group_weights.sum()
    pn = None
    if denom_net != 0:
        pn = numerator / denom_net   # note: if some tickers missing on a day, denominator is constant net weight but numerator uses available tickers

    portfolio_gross[pd.to_datetime(trade_date)] = pg
    if pn is not None:
        portfolio_net[pd.to_datetime(trade_date)] = pn

# ---------- DataFrames (index = day offsets) ----------
portfolio_gross_df = pd.DataFrame(portfolio_gross, index=day_idx).sort_index(axis=1)
portfolio_net_df   = pd.DataFrame(portfolio_net, index=day_idx) if portfolio_net else None

if portfolio_gross_df.empty:
    raise ValueError("No portfolio curves were produced (empty result). Check your file and that trade dates / price columns are correct.")

# ---------- average curves ----------
avg_gross = portfolio_gross_df.mean(axis=1, skipna=True)
avg_net = portfolio_net_df.mean(axis=1, skipna=True) if (portfolio_net_df is not None and not portfolio_net_df.empty) else None

# ---------- plotting (daily x-axis) ----------
plt.figure(figsize=(13,8))

# plot each trade-date curve (light)
max_legend = 30  # to avoid overly large legends; show at most this many in legend
legend_labels = []
for i, col in enumerate(portfolio_gross_df.columns):
    y = portfolio_gross_df[col].values
    plt.plot(day_idx, y, alpha=0.25, linewidth=1, color='gray')
    if i < max_legend:
        legend_labels.append(str(col.date()))
# overlay average gross-normalized (bold)
plt.plot(day_idx, avg_gross.values, color='black', linewidth=2.5, label='Avg (gross-normalized)')
# overlay avg net if available
if avg_net is not None:
    plt.plot(day_idx, avg_net.values, color='red', linewidth=1.5, linestyle='--', label='Avg (net-normalized)')

plt.axhline(0, color='black', linestyle='--', linewidth=0.8)
plt.xlabel('Days since trade date (t0)')
plt.ylabel('Return (vs t0)')
plt.title('Portfolio returns by trade date (each line = trades executed that date)')
plt.grid(True)

# build legend: show only the average + up to N trade-dates to avoid clutter
handles, labels = plt.gca().get_legend_handles_labels()
# manually add a sample of trade dates (optional)
if len(portfolio_gross_df.columns) <= max_legend:
    # show all trade-date names in legend (may be many)
    plt.legend()
else:
    # show only avg lines in legend + note
    plt.legend(handles=handles, labels=labels)
    plt.text(0.99, 0.02, f"Plotted {len(portfolio_gross_df.columns)} trade dates; individual dates shown as faint gray lines.",
             transform=plt.gca().transAxes, ha='right', va='bottom', fontsize=9, alpha=0.8)

plt.show()

# ---------- optional: save portfolio curves to CSV ----------
portfolio_gross_df.to_csv("portfolio_curves_gross_by_trade_date.csv", index_label="day_offset")
if portfolio_net_df is not None:
    portfolio_net_df.to_csv("portfolio_curves_net_by_trade_date.csv", index_label="day_offset")

print("Done. Saved portfolio_curves_gross_by_trade_date.csv (and net file if applicable).")



----


import pandas as pd

# ------------------------------
# Simplified fast entry criteria
# (based on MSCI/FTSE published methodology — approximate, not official)
# ------------------------------

# MSCI EM / World fast entry thresholds (USD m)
MSCI_LARGE_CAP_MIN = 2600   # ~USD 2.6bn for EM (varies by region)
MSCI_STANDARD_MIN  = 1300   # ~USD 1.3bn (for Standard Index, developed markets)
MSCI_MIN_FREEFLOAT = 0.15   # 15% free float minimum

# FTSE GEIS fast entry thresholds (USD m)
FTSE_ALLWORLD_MIN  = 1600   # ~USD 1.6bn
FTSE_MIN_FREEFLOAT = 0.15

# ------------------------------
# Screening function
# ------------------------------

def check_fast_entry(name, market_cap_usd, free_float_pct, country, avg_daily_value_usd=None):
    """
    Parameters
    ----------
    name : str
        IPO name
    market_cap_usd : float
        Expected full market cap at IPO (USD)
    free_float_pct : float
        Free float percentage (0-1)
    country : str
        Listing country (used for classification, not detailed here)
    avg_daily_value_usd : float
        Optional: expected ADV for liquidity check
    
    Returns
    -------
    dict with MSCI and FTSE fast entry checks
    """
    results = {}

    # MSCI
    ff_mcap = market_cap_usd * free_float_pct
    msci_pass = (
        free_float_pct >= MSCI_MIN_FREEFLOAT and
        (ff_mcap >= MSCI_LARGE_CAP_MIN or ff_mcap >= MSCI_STANDARD_MIN)
    )
    results["MSCI_fast_entry"] = msci_pass
    results["MSCI_ff_mcap_usd"] = round(ff_mcap, 1)

    # FTSE
    ftse_pass = (
        free_float_pct >= FTSE_MIN_FREEFLOAT and
        ff_mcap >= FTSE_ALLWORLD_MIN
    )
    results["FTSE_fast_entry"] = ftse_pass
    results["FTSE_ff_mcap_usd"] = round(ff_mcap, 1)

    # Liquidity (optional)
    if avg_daily_value_usd is not None:
        results["Liquidity_check"] = avg_daily_value_usd > 1e6  # crude rule of thumb
    
    results["IPO_name"] = name
    results["Country"] = country
    return results

# ------------------------------
# Example usage
# ------------------------------

ipo_list = [
    {"name": "ExampleTech", "mcap": 5000, "ff": 0.25, "country": "US", "adv": 50e6},
    {"name": "SmallBank",   "mcap": 800,  "ff": 0.30, "country": "UK", "adv": 5e6},
    {"name": "FrontierCo",  "mcap": 2000, "ff": 0.10, "country": "India", "adv": 2e6},
]

results = [check_fast_entry(i["name"], i["mcap"], i["ff"], i["country"], i["adv"]) for i in ipo_list]
df = pd.DataFrame(results)
print(df)


----

# --- IPO Fast Entry Analyzer for FTSE GEIS and MSCI GIMI ---
# Run this cell in a Jupyter Notebook

from typing import Dict, Any
import pandas as pd

class IndexParams:
    def __init__(self,
                 name: str,
                 min_fmc: float,
                 min_investable_cap: float,
                 max_rank: int,
                 require_liquidity: bool = True,
                 min_turnover: float = None,
                 min_trade_frequency: float = None,
                 require_free_float: float = 0.15,
                 require_foreign_room: float = 0.15):
        self.name = name
        self.min_fmc = min_fmc
        self.min_investable_cap = min_investable_cap
        self.max_rank = max_rank
        self.require_liquidity = require_liquidity
        self.min_turnover = min_turnover
        self.min_trade_frequency = min_trade_frequency
        self.require_free_float = require_free_float
        self.require_foreign_room = require_foreign_room

class IPOCandidate:
    def __init__(self, offering_shares: float,
                 first_day_close_price: float,
                 full_market_cap: float,
                 free_float_ratio: float,
                 rank: int,
                 turnover: float = None,
                 trade_freq: float = None,
                 foreign_room: float = None):
        self.offering_shares = offering_shares
        self.first_day_close_price = first_day_close_price
        self.full_market_cap = full_market_cap
        self.free_float_ratio = free_float_ratio
        self.rank = rank
        self.turnover = turnover
        self.trade_freq = trade_freq
        self.foreign_room = foreign_room

def check_fast_entry(ipoc: IPOCandidate, params: IndexParams) -> Dict[str, Any]:
    results = {}
    results['Index'] = params.name

    # FMC (Fast IPO Market Cap)
    fmc = ipoc.offering_shares * ipoc.first_day_close_price
    results['FMC'] = fmc
    results['Pass FMC'] = fmc >= params.min_fmc

    # Rank
    results['Rank'] = ipoc.rank
    results['Pass Rank'] = (ipoc.rank <= params.max_rank)

    # Investable cap
    investable_cap = ipoc.full_market_cap * ipoc.free_float_ratio
    results['Investable Cap'] = investable_cap
    results['Pass Investable'] = investable_cap >= params.min_investable_cap

    # Free float
    results['Free Float %'] = ipoc.free_float_ratio
    results['Pass Free Float'] = ipoc.free_float_ratio >= params.require_free_float

    # Liquidity
    if params.require_liquidity:
        ok_turnover = True
        ok_freq = True
        if params.min_turnover is not None:
            ok_turnover = (ipoc.turnover is not None and ipoc.turnover >= params.min_turnover)
        if params.min_trade_frequency is not None:
            ok_freq = (ipoc.trade_freq is not None and ipoc.trade_freq >= params.min_trade_frequency)
        results['Pass Liquidity'] = ok_turnover and ok_freq
    else:
        results['Pass Liquidity'] = True

    # Foreign room
    results['Foreign Room %'] = ipoc.foreign_room
    results['Pass Foreign Room'] = (ipoc.foreign_room is None 
                                    or ipoc.foreign_room >= params.require_foreign_room)

    # Final decision
    results['Fast Entry Eligible'] = (results['Pass FMC']
                                      and results['Pass Rank']
                                      and results['Pass Investable']
                                      and results['Pass Free Float']
                                      and results['Pass Liquidity']
                                      and results['Pass Foreign Room'])
    return results

# --------------------------
# FTSE GEIS and MSCI GIMI Parameters
# --------------------------

ftse_geis_params = IndexParams(
    name = "FTSE GEIS Fast Entry",
    min_fmc = 2_000_000_000,   # USD 2B investable market cap
    min_investable_cap = 2_000_000_000,
    max_rank = 500,            # top 500 global
    require_liquidity = False, # liquidity often waived for IPOs
    require_free_float = 0.15,
    require_foreign_room = 0.15
)

msci_gimi_params = IndexParams(
    name = "MSCI GIMI Fast Entry",
    min_fmc = 1_800_000_000,   # example: 1.8× cutoff for Standard index
    min_investable_cap = 1_800_000_000,
    max_rank = 500,
    require_liquidity = True,
    min_turnover = 30_000_000,
    min_trade_frequency = 0.75,
    require_free_float = 0.15,
    require_foreign_room = 0.15
)

# --------------------------
# Example IPO Candidate
# --------------------------

ipo_test = IPOCandidate(
    offering_shares = 70_000_000,
    first_day_close_price = 30.0,
    full_market_cap = 6_000_000_000,
    free_float_ratio = 0.2,   # 20% float
    rank = 150,
    turnover = 100_000_000,
    trade_freq = 0.9,
    foreign_room = 0.25
)

# --------------------------
# Run checks and display in DataFrame
# --------------------------

results = []
for idx_params in [ftse_geis_params, msci_gimi_params]:
    results.append(check_fast_entry(ipo_test, idx_params))

df = pd.DataFrame(results)
df


-----


# Portfolio Trading Catch-Up Notes

## 1️⃣ Opening / Context
- “Thanks for taking the time — I wanted to check in on where I stand and what I should be focusing on.”
- Goal: understand expectations, focus areas, and how to accelerate learning.

---

## 2️⃣ Expectations & Feedback
- What do you expect from me at this stage on the desk?
- What does ‘excelling’ look like for a junior trader here?
- Any feedback on how I’ve been doing so far?

---

## 3️⃣ Focus Areas & Development
- What should my main focus be over the next 6–12 months?
- Which skills or habits set apart juniors who go on to become strong risk-takers?
- Are there particular markets, sectors, or products I should deepen knowledge in?

---

## 4️⃣ Skills / Progress Update
- I’ve been improving my Python skills — building tools for analytics and workflow efficiency.
- (Optional) I’ve been studying factor risks and portfolio exposures, not just single names.

---

## 5️⃣ Pathway Into Trading / Risk
- What does the pathway look like from junior trader → running risk → senior trader?
- When do juniors usually start getting involved in trading decisions or managing small books?
- Are there exams or exchange registrations I should plan for?
- Already done: FCA competency (if UK). Next: CISI / IMC / Series 7/57 depending on region.

---

## 6️⃣ Big Picture / Desk Insight
- Biggest trends/challenges in portfolio trading right now?
- How is the desk evolving — more electronic, more risk transfer, or client facilitation?

---

## 7️⃣ Structural ETF Risks / Shorting
- **Leveraged/Inverse ETFs:**  
  - Decay comes from daily rebalancing → volatility drag + compounding.  
  - Shorting risky because large trending moves can explode against you.  
  - Example: 2x ETF, index +10% then −10% → ETF loses 4%, short payoff not symmetric.
- **Commodity / VIX ETFs:**  
  - Decay comes from roll yield (contango) → repeated selling cheap front-month, buying expensive next-month.  
  - Tail risk is large, borrow costs can be high.  

- Floor explanation:  
  > “For equity leveraged ETFs, drag is from daily rebalancing. For commodity/VIX ETFs, it’s from roll yield. Both bleed over time, but big moves can destroy shorts quickly — that’s why it’s not a free lunch.”

---

## 8️⃣ Portfolio Risk Metrics
- **VaR (Value at Risk):** Maximum expected loss at a confidence level over a horizon.  
  - Example: 1-day VaR $1M at 95% → 95% chance losses ≤ $1M.
- **Types of VaR:** Parametric / Historical / Monte Carlo
- **Related Metrics:** Expected Shortfall (CVaR), stress tests, scenario analysis, incremental/marginal VaR, factor sensitivities.
- **Use on desk:** Position sizing, hedging, internal risk monitoring, trade approvals.
- **Caveats:** Assumes normal conditions, misses fat tails. Must complement with stress testing.

- Floor explanation:  
  > “VaR gives a confidence-level estimate of maximum expected loss. On the desk, it guides position sizing and exposure, but we complement it with stress tests since it doesn’t capture tail events.”

---

## 9️⃣ Exams / Registrations
- **US:** SIE → Series 7 → Series 57 (+ Series 63 if client-facing across states)
- **UK / Europe:** FCA competency done, then CISI exams or IMC; internal sign-offs for exchange registration.
- **Asia:** HKSI (LE 1/7/8), SGX CMFAS modules.
- **Exchange registration:** Usually handled by compliance; includes online modules / attestations.  

- Smart 1:1 question:  
  > “I’ve completed FCA competency. When do juniors usually get exchange registrations signed off, e.g., LSE or other venues?”

---

## 🔟 Mistakes to Avoid Early
- Thinking in single names instead of portfolio/factor exposures.
- Overtrading or chasing P&L.
- Ignoring sector/factor concentration in baskets.
- Ignoring liquidity or operational risks.
- Over-promising fills or trading ability to seniors or clients.

---

## 1️⃣1️⃣ Questions to Ask Your Manager
1. What do you expect from me at this stage, and what does excelling look like?  
2. Where should I focus my learning/development over the next 6–12 months?  
3. Are there particular markets, sectors, or products you’d like me to deepen knowledge in?  
4. Feedback: anything I should start doing differently tomorrow?  
5. Pathway: what does progression look like from junior → running risk?  
6. Skills: which skills/habits set apart juniors who succeed?  
7. Exams/Registrations: what should I plan for and when?  
8. Big picture: biggest trends or challenges on the desk right now?

---

## 1️⃣2️⃣ Wrap / Close
- “Thanks for the guidance — this is really helpful. If there’s one thing I should start doing differently or better immediately, what would it be?”  
- “If I want to prepare to be useful when opportunities to take risk come up, what’s the single most important thing I should do?”
