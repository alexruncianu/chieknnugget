import pandas as pd
from datetime import datetime, timedelta
import os

# ---------- CONFIG ----------
# Directory where your MSCI files live
DATA_DIR = "."  # or specify your Faast storage path

# Required columns
REQUIRED_COLS = ["ticker", "company name", "change", "notional to trade", "comments", "conviction"]

# ---------- HELPER FUNCTIONS ----------

def load_and_clean(file_path):
    """Load CSV and standardize column names."""
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip().str.lower()

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"{file_path} is missing columns: {missing}")

    for col in REQUIRED_COLS:
        if df[col].dtype == object:
            df[col] = df[col].fillna("").astype(str).str.strip()
    return df[REQUIRED_COLS]


def compare_snapshots(df_yest, df_today):
    """Compare two snapshots and return adds, deletes, and changes."""
    y_tickers = set(df_yest["ticker"])
    t_tickers = set(df_today["ticker"])

    additions = df_today[df_today["ticker"].isin(t_tickers - y_tickers)]
    deletions = df_yest[df_yest["ticker"].isin(y_tickers - t_tickers)]

    # Common tickers for modifications
    common_today = df_today[df_today["ticker"].isin(y_tickers & t_tickers)]
    merged = common_today.merge(df_yest, on="ticker", suffixes=("_today", "_yest"))

    changes = []
    for _, row in merged.iterrows():
        diffs = []
        if row["notional to trade_today"] != row["notional to trade_yest"]:
            diffs.append(
                f"Notional {row['notional to trade_yest']} → {row['notional to trade_today']}"
            )
        if row["conviction_today"].lower() != row["conviction_yest"].lower():
            diffs.append(
                f"Conviction {row['conviction_yest']} → {row['conviction_today']}"
            )
        if row["comments_today"] != row["comments_yest"]:
            diffs.append("Comments updated")
        if diffs:
            changes.append(
                f"- {row['ticker']} ({row['company name_today']}): " + ", ".join(diffs)
            )

    # Watchlist summary
    watchlist = df_today[df_today["conviction"].str.lower() == "watchlist"]

    return additions, deletions, changes, watchlist


def format_summary(region, additions, deletions, changes, watchlist):
    """Format readable text summary."""
    lines = [f"📊 {region} Summary"]

    if not additions.empty:
        lines.append("➕ Additions:")
        for _, r in additions.iterrows():
            lines.append(f"- {r['ticker']} ({r['company name']}): {r['change']}, Notional {r['notional to trade']}, Conviction: {r['conviction']}")
    else:
        lines.append("➕ Additions: None")

    if not deletions.empty:
        lines.append("➖ Deletions:")
        for _, r in deletions.iterrows():
            lines.append(f"- {r['ticker']} ({r['company name']})")
    else:
        lines.append("➖ Deletions: None")

    if changes:
        lines.append("🔁 Changes:")
        lines.extend(changes)
    else:
        lines.append("🔁 Changes: None")

    if not watchlist.empty:
        lines.append("👀 Watchlist:")
        for _, r in watchlist.iterrows():
            lines.append(f"- {r['ticker']} ({r['company name']})")
    else:
        lines.append("👀 Watchlist: None")

    lines.append("-" * 50)
    return "\n".join(lines)


def generate_daily_summary():
    today = datetime.now()
    yesterday = today - timedelta(days=1)

    today_str = today.strftime("%Y%m%d")
    yest_str = yesterday.strftime("%Y%m%d")

    regions = ["EM", "DM"]
    summaries = []

    for region in regions:
        today_file = os.path.join(DATA_DIR, f"{region}_{today_str}.csv")
        yest_file = os.path.join(DATA_DIR, f"{region}_{yest_str}.csv")

        if not os.path.exists(today_file) or not os.path.exists(yest_file):
            summaries.append(f"⚠️ {region}: Missing files ({today_file} or {yest_file})\n" + "-" * 50)
            continue

        df_yest = load_and_clean(yest_file)
        df_today = load_and_clean(today_file)

        additions, deletions, changes, watchlist = compare_snapshots(df_yest, df_today)
        summary_text = format_summary(region, additions, deletions, changes, watchlist)
        summaries.append(summary_text)

    full_summary = f"MSCI Daily Summary ({today.strftime('%d %b %Y')})\n\n" + "\n".join(summaries)
    print(full_summary)
    return full_summary


if __name__ == "__main__":
    generate_daily_summary()

"""
===========================================================
  Factor Leg Leverage Factor Score + Company Frequency Analysis
===========================================================

Inputs:
- factor_members.csv  -> columns = factor legs, rows = tickers
- leverage_scores.csv -> columns = same factor legs, rows = leverage factor scores

Outputs:
- CSV files:
    • top_leverage_factor_score_companies.csv
    • bottom_leverage_factor_score_companies.csv
    • factor_leg_average_leverage_factor_score.csv
    • most_common_companies_across_factors.csv
- Visual charts for leverage factor scores and overlap
===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === Config ===
WORKDIR = os.getcwd()
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

TOP_N_COMPANIES = 25
TOP_N_LEGS = 15

# === Load CSVs ===
members = pd.read_csv(MEMBERS_CSV, dtype=str)
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str)

# === Clean factor names ===
def clean_factor_name(name):
    name = str(name).strip().upper()
    name = name.replace(" ", "_").replace("-", "_")
    if name.endswith("_1"):
        name = name[:-2]
    elif name.endswith("1"):
        name = name[:-1]
    name = ''.join(ch for ch in name if ch.isalnum() or ch == "_")
    return name

members.columns = [clean_factor_name(c) for c in members.columns]
leverage.columns = [clean_factor_name(c) for c in leverage.columns]

# Remove duplicate columns
members = members.loc[:, ~members.columns.duplicated()]
leverage = leverage.loc[:, ~leverage.columns.duplicated()]

# Keep only common columns
common_cols = sorted(set(members.columns) & set(leverage.columns))
members = members[common_cols].applymap(lambda x: x.strip().upper() if isinstance(x, str) else x)
leverage = leverage[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Align row counts
max_rows = max(members.shape[0], leverage.shape[0])
members = members.reindex(range(max_rows))
leverage = leverage.reindex(range(max_rows))

# === Stack and merge ===
members_stack = members.stack().reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack().reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Factor_Score"]

df = pd.merge(members_stack, leverage_stack, on=["row_idx", "Factor_Leg"], how="left")
df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()
df["Leverage_Factor_Score"] = pd.to_numeric(df["Leverage_Factor_Score"].astype(str).str.replace(",", ""), errors="coerce")
df = df.dropna(subset=["Ticker", "Leverage_Factor_Score"])
df = df.drop_duplicates(subset=["Ticker", "Factor_Leg"])
df["Factor_Leg"] = df["Factor_Leg"].str.strip().str.upper()

# === Compute summaries ===
# Individual companies
ticker_group = (
    df.groupby("Ticker")
    .agg(
        Mean_Leverage_Factor_Score=("Leverage_Factor_Score", "mean"),
        Count_Legs=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Mean_Leverage_Factor_Score", ascending=False)
)

# Factor leg averages
leg_stats = (
    df.groupby("Factor_Leg")
    .agg(
        Avg_Leverage_Factor_Score=("Leverage_Factor_Score", "mean"),
        Median_Leverage_Factor_Score=("Leverage_Factor_Score", "median"),
        Count_Tickers=("Ticker", "nunique")
    )
    .reset_index()
)
leg_stats = leg_stats.drop_duplicates(subset=["Factor_Leg"]).sort_values("Avg_Leverage_Factor_Score", ascending=False).reset_index(drop=True)

# === Factor overlap matrix ===
legs = sorted(df["Factor_Leg"].unique())
leg_sets = {leg: set(df.loc[df["Factor_Leg"] == leg, "Ticker"]) for leg in legs}
overlap = pd.DataFrame(index=legs, columns=legs, dtype=int)
for a in legs:
    for b in legs:
        if a != b:
            overlap.loc[a, b] = len(leg_sets[a].intersection(leg_sets[b]))
        else:
            overlap.loc[a, b] = np.nan

top_legs = overlap.sum(axis=1).sort_values(ascending=False).head(TOP_N_LEGS).index.tolist()
overlap_top = overlap.loc[top_legs, top_legs]

# === Visualization 1: Factor Legs by Avg Leverage Factor Score ===
plt.figure(figsize=(12, max(6, 0.25*len(leg_stats))))
plt.barh(leg_stats["Factor_Leg"], leg_stats["Avg_Leverage_Factor_Score"], color="skyblue", edgecolor="k")
plt.xlabel("Average Leverage Factor Score")
plt.title("All Factor Legs by Average Leverage Factor Score")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# === Visualization 2: Top & Bottom Companies ===
top_companies = ticker_group.head(TOP_N_COMPANIES)
bottom_companies = ticker_group.tail(TOP_N_COMPANIES).sort_values("Mean_Leverage_Factor_Score")

fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)
axes[0].barh(top_companies["Ticker"], top_companies["Mean_Leverage_Factor_Score"], color="crimson", edgecolor="k")
axes[0].set_title(f"Top {TOP_N_COMPANIES} Most Leveraged Companies (Factor Score)")
axes[0].invert_yaxis()
axes[0].set_xlabel("Average Leverage Factor Score")

axes[1].barh(bottom_companies["Ticker"], bottom_companies["Mean_Leverage_Factor_Score"], color="lightgreen", edgecolor="k")
axes[1].set_title(f"Top {TOP_N_COMPANIES} Least Leveraged Companies (Factor Score)")
axes[1].set_xlabel("Average Leverage Factor Score")

plt.tight_layout()
plt.show()

# === Visualization 3: Factor Overlap Heatmap ===
plt.figure(figsize=(8, 6))
plt.imshow(overlap_top.values, cmap="Blues", interpolation="nearest")
plt.colorbar(label="Number of Shared Companies")
plt.xticks(range(len(top_legs)), top_legs, rotation=90)
plt.yticks(range(len(top_legs)), top_legs)
plt.title(f"Factor Overlap Matrix (Top {TOP_N_LEGS} Most Overlapping)")
plt.tight_layout()
plt.show()

# === New: Most Common Companies Across Factors ===
most_common = (
    df.groupby("Ticker")
    .agg(Count_Legs=("Factor_Leg", "nunique"), Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x)))))
    .reset_index()
    .sort_values("Count_Legs", ascending=False)
)

most_common_top = most_common.head(25)

# === Export CSVs ===
top_companies.to_csv(os.path.join(WORKDIR, "top_leverage_factor_score_companies.csv"), index=False)
bottom_companies.to_csv(os.path.join(WORKDIR, "bottom_leverage_factor_score_companies.csv"), index=False)
leg_stats.to_csv(os.path.join(WORKDIR, "factor_leg_average_leverage_factor_score.csv"), index=False)
most_common_top.to_csv(os.path.join(WORKDIR, "most_common_companies_across_factors.csv"), index=False)

# === Print summary tables ===
print("\nTop 10 Highest-Leverage Factor Score Companies:\n")
print(top_companies.head(10).to_string(index=False))

print("\nTop 10 Lowest-Leverage Factor Score Companies:\n")
print(bottom_companies.head(10).to_string(index=False))

print("\nTop 10 Factor Legs by Average Leverage Factor Score:\n")
print(leg_stats.head(10).to_string(index=False, formatters={"Avg_Leverage_Factor_Score":"{:.2f}".format}))

print("\nBottom 10 Factor Legs by Average Leverage Factor Score:\n")
print(leg_stats.tail(10).sort_values("Avg_Leverage_Factor_Score").to_string(index=False, formatters={"Avg_Leverage_Factor_Score":"{:.2f}".format}))

print("\nTop 15 Companies Appearing in the Most Factor Legs:\n")
print(most_common_top.to_string(index=False))

"""
===========================================================
  Factor Leg Leverage Factor Score Analysis (Final)
===========================================================

Inputs:
- factor_members.csv  -> columns = factor legs, rows = tickers
- leverage_scores.csv -> columns = same factor legs, rows = leverage factor scores

Outputs:
- CSV files:
    • top_leverage_factor_score_companies.csv
    • bottom_leverage_factor_score_companies.csv
    • factor_leg_average_leverage_factor_score.csv
- Charts for all factors, top/bottom companies, factor overlap
===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === Config ===
WORKDIR = os.getcwd()
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

TOP_N_COMPANIES = 25
TOP_N_LEGS = 15

# === Load CSVs ===
members = pd.read_csv(MEMBERS_CSV, dtype=str)
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str)

# === Clean factor names ===
def clean_factor_name(name):
    name = str(name).strip().upper()
    name = name.replace(" ", "_").replace("-", "_")
    if name.endswith("_1"):
        name = name[:-2]
    elif name.endswith("1"):
        name = name[:-1]
    name = ''.join(ch for ch in name if ch.isalnum() or ch == "_")
    return name

members.columns = [clean_factor_name(c) for c in members.columns]
leverage.columns = [clean_factor_name(c) for c in leverage.columns]

# Remove exact duplicate columns
members = members.loc[:, ~members.columns.duplicated()]
leverage = leverage.loc[:, ~leverage.columns.duplicated()]

# Keep only common columns
common_cols = sorted(set(members.columns) & set(leverage.columns))
members = members[common_cols].applymap(lambda x: x.strip().upper() if isinstance(x, str) else x)
leverage = leverage[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Align row counts
max_rows = max(members.shape[0], leverage.shape[0])
members = members.reindex(range(max_rows))
leverage = leverage.reindex(range(max_rows))

# === Stack and merge ===
members_stack = members.stack().reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack().reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Factor_Score"]

df = pd.merge(members_stack, leverage_stack, on=["row_idx", "Factor_Leg"], how="left")
df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()
df["Leverage_Factor_Score"] = pd.to_numeric(df["Leverage_Factor_Score"].astype(str).str.replace(",", ""), errors="coerce")
df = df.dropna(subset=["Ticker", "Leverage_Factor_Score"])
df = df.drop_duplicates(subset=["Ticker", "Factor_Leg"])
df["Factor_Leg"] = df["Factor_Leg"].str.strip().str.upper()

# === Compute summaries ===
# Individual companies
ticker_group = (
    df.groupby("Ticker")
    .agg(
        Mean_Leverage_Factor_Score=("Leverage_Factor_Score", "mean"),
        Count_Legs=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Mean_Leverage_Factor_Score", ascending=False)
)

# Factor leg averages
leg_stats = (
    df.groupby("Factor_Leg")
    .agg(
        Avg_Leverage_Factor_Score=("Leverage_Factor_Score", "mean"),
        Median_Leverage_Factor_Score=("Leverage_Factor_Score", "median"),
        Count_Tickers=("Ticker", "nunique")
    )
    .reset_index()
)
leg_stats = leg_stats.drop_duplicates(subset=["Factor_Leg"]).sort_values("Avg_Leverage_Factor_Score", ascending=False).reset_index(drop=True)

# Factor overlap matrix (only shared tickers)
legs = sorted(df["Factor_Leg"].unique())
leg_sets = {leg: set(df.loc[df["Factor_Leg"] == leg, "Ticker"]) for leg in legs}
overlap = pd.DataFrame(index=legs, columns=legs, dtype=int)
for a in legs:
    for b in legs:
        if a != b:
            overlap.loc[a, b] = len(leg_sets[a].intersection(leg_sets[b]))
        else:
            overlap.loc[a, b] = np.nan

# Keep top overlapping legs
top_legs = overlap.sum(axis=1).sort_values(ascending=False).head(TOP_N_LEGS).index.tolist()
overlap_top = overlap.loc[top_legs, top_legs]

# === Visualization 1: All factor legs by average leverage factor score ===
plt.figure(figsize=(12, max(6, 0.25*len(leg_stats))))
plt.barh(leg_stats["Factor_Leg"], leg_stats["Avg_Leverage_Factor_Score"], color="skyblue", edgecolor="k")
plt.xlabel("Average Leverage Factor Score")
plt.title("All Factor Legs by Average Leverage Factor Score")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# === Visualization 2: Top & Bottom Companies ===
top_companies = ticker_group.head(TOP_N_COMPANIES)
bottom_companies = ticker_group.tail(TOP_N_COMPANIES).sort_values("Mean_Leverage_Factor_Score")

fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

axes[0].barh(top_companies["Ticker"], top_companies["Mean_Leverage_Factor_Score"], color="crimson", edgecolor="k")
axes[0].set_title(f"Top {TOP_N_COMPANIES} Most Leveraged Companies (Factor Score)")
axes[0].invert_yaxis()
axes[0].set_xlabel("Average Leverage Factor Score")

axes[1].barh(bottom_companies["Ticker"], bottom_companies["Mean_Leverage_Factor_Score"], color="lightgreen", edgecolor="k")
axes[1].set_title(f"Top {TOP_N_COMPANIES} Least Leveraged Companies (Factor Score)")
axes[1].set_xlabel("Average Leverage Factor Score")

plt.tight_layout()
plt.show()

# === Visualization 3: Factor Overlap Heatmap ===
plt.figure(figsize=(8, 6))
plt.imshow(overlap_top.values, cmap="Blues", interpolation="nearest")
plt.colorbar(label="Number of Shared Companies")
plt.xticks(range(len(top_legs)), top_legs, rotation=90)
plt.yticks(range(len(top_legs)), top_legs)
plt.title(f"Factor Overlap Matrix (Top {TOP_N_LEGS} Most Overlapping)")
plt.tight_layout()
plt.show()

# === Export CSVs ===
top_companies.to_csv(os.path.join(WORKDIR, "top_leverage_factor_score_companies.csv"), index=False)
bottom_companies.to_csv(os.path.join(WORKDIR, "bottom_leverage_factor_score_companies.csv"), index=False)
leg_stats.to_csv(os.path.join(WORKDIR, "factor_leg_average_leverage_factor_score.csv"), index=False)

# === Summary Tables ===
print("\nTop 10 Highest-Leverage Factor Score Companies:\n")
print(top_companies.head(10).to_string(index=False))

print("\nTop 10 Lowest-Leverage Factor Score Companies:\n")
print(bottom_companies.head(10).to_string(index=False))

print("\nTop 10 Factor Legs by Average Leverage Factor Score:\n")
print(leg_stats.head(10).to_string(index=False, formatters={"Avg_Leverage_Factor_Score":"{:.2f}".format}))

print("\nBottom 10 Factor Legs by Average Leverage Factor Score:\n")
print(leg_stats.tail(10).sort_values("Avg_Leverage_Factor_Score").to_string(index=False, formatters={"Avg_Leverage_Factor_Score":"{:.2f}".format}))

"""
===========================================================
  Factor Leg Leverage Analysis (Top & Bottom Companies + All Factors)
===========================================================

Inputs:
- factor_members.csv  -> columns = factor legs, rows = tickers
- leverage_scores.csv -> columns = same factor legs, rows = leverage scores

Outputs & Features:
- Deduplicates factor names (removes duplicates and trailing _1)
- Shows top and bottom leveraged factor legs
- Shows top and bottom individual companies
- Full bar chart for all factor legs by average leverage
- Factor overlap heatmap (cleaned)
===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === Config ===
WORKDIR = os.getcwd()
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

TOP_N_COMPANIES = 25
TOP_N_LEGS = 15

# === Load CSVs ===
members = pd.read_csv(MEMBERS_CSV, dtype=str)
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str)

# === Clean factor names ===
def clean_factor_name(name):
    name = str(name).strip().upper()
    name = name.replace(" ", "_").replace("-", "_")
    if name.endswith("_1"):
        name = name[:-2]
    elif name.endswith("1"):
        name = name[:-1]
    name = ''.join(ch for ch in name if ch.isalnum() or ch == "_")
    return name

members.columns = [clean_factor_name(c) for c in members.columns]
leverage.columns = [clean_factor_name(c) for c in leverage.columns]

# Remove exact duplicate columns
members = members.loc[:, ~members.columns.duplicated()]
leverage = leverage.loc[:, ~leverage.columns.duplicated()]

# Keep only common columns
common_cols = sorted(set(members.columns) & set(leverage.columns))
members = members[common_cols].applymap(lambda x: x.strip().upper() if isinstance(x, str) else x)
leverage = leverage[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Align row counts
max_rows = max(members.shape[0], leverage.shape[0])
members = members.reindex(range(max_rows))
leverage = leverage.reindex(range(max_rows))

# === Stack and merge ===
members_stack = members.stack().reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack().reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Score"]

df = pd.merge(members_stack, leverage_stack, on=["row_idx", "Factor_Leg"], how="left")
df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()
df["Leverage_Score"] = pd.to_numeric(df["Leverage_Score"].astype(str).str.replace(",", ""), errors="coerce")
df = df.dropna(subset=["Ticker", "Leverage_Score"])
df = df.drop_duplicates(subset=["Ticker", "Factor_Leg"])
df["Factor_Leg"] = df["Factor_Leg"].str.strip().str.upper()

# === Compute summaries ===
# Individual companies
ticker_group = (
    df.groupby("Ticker")
    .agg(
        Mean_Leverage=("Leverage_Score", "mean"),
        Count_Legs=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Mean_Leverage", ascending=False)
)

# Factor leg averages
leg_stats = (
    df.groupby("Factor_Leg")
    .agg(
        Avg_Leverage=("Leverage_Score", "mean"),
        Median_Leverage=("Leverage_Score", "median"),
        Count_Tickers=("Ticker", "nunique")
    )
    .reset_index()
)
leg_stats = leg_stats.drop_duplicates(subset=["Factor_Leg"]).sort_values("Avg_Leverage", ascending=False).reset_index(drop=True)

# Factor overlap matrix (only based on shared tickers)
legs = sorted(df["Factor_Leg"].unique())
leg_sets = {leg: set(df.loc[df["Factor_Leg"] == leg, "Ticker"]) for leg in legs}
overlap = pd.DataFrame(index=legs, columns=legs, dtype=int)
for a in legs:
    for b in legs:
        if a != b:
            overlap.loc[a, b] = len(leg_sets[a].intersection(leg_sets[b]))
        else:
            overlap.loc[a, b] = np.nan

# Keep top overlapping legs for readability
top_legs = overlap.sum(axis=1).sort_values(ascending=False).head(TOP_N_LEGS).index.tolist()
overlap_top = overlap.loc[top_legs, top_legs]

# === Visualization 1: All factor legs by average leverage ===
plt.figure(figsize=(12, max(6, 0.25*len(leg_stats))))
plt.barh(leg_stats["Factor_Leg"], leg_stats["Avg_Leverage"], color="skyblue", edgecolor="k")
plt.xlabel("Average Leverage Score")
plt.title("All Factor Legs by Average Leverage")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# === Visualization 2: Top & Bottom Companies ===
top_companies = ticker_group.head(TOP_N_COMPANIES)
bottom_companies = ticker_group.tail(TOP_N_COMPANIES).sort_values("Mean_Leverage")

fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

axes[0].barh(top_companies["Ticker"], top_companies["Mean_Leverage"], color="crimson", edgecolor="k")
axes[0].set_title(f"Top {TOP_N_COMPANIES} Most Leveraged Companies")
axes[0].invert_yaxis()
axes[0].set_xlabel("Average Leverage")

axes[1].barh(bottom_companies["Ticker"], bottom_companies["Mean_Leverage"], color="lightgreen", edgecolor="k")
axes[1].set_title(f"Top {TOP_N_COMPANIES} Least Leveraged Companies")
axes[1].set_xlabel("Average Leverage")

plt.tight_layout()
plt.show()

# === Visualization 3: Factor Overlap Heatmap ===
plt.figure(figsize=(8, 6))
plt.imshow(overlap_top.values, cmap="Blues", interpolation="nearest")
plt.colorbar(label="Number of Shared Companies")
plt.xticks(range(len(top_legs)), top_legs, rotation=90)
plt.yticks(range(len(top_legs)), top_legs)
plt.title(f"Factor Overlap Matrix (Top {TOP_N_LEGS} Most Overlapping)")
plt.tight_layout()
plt.show()

# === Summary Tables ===
print("\nTop 10 Highest-Leverage Companies:\n")
print(ticker_group.head(10).to_string(index=False))

print("\nTop 10 Lowest-Leverage Companies:\n")
print(ticker_group.tail(10).sort_values("Mean_Leverage").to_string(index=False))

print("\nTop 10 Factor Legs by Average Leverage:\n")
print(leg_stats.head(10).to_string(index=False, formatters={"Avg_Leverage":"{:.2f}".format}))

print("\nBottom 10 Factor Legs by Average Leverage:\n")
print(leg_stats.tail(10).sort_values("Avg_Leverage").to_string(index=False, formatters={"Avg_Leverage":"{:.2f}".format}))

"""
===========================================================
  Factor Leg Leverage & Overlap + Company Co-Occurrence Analysis
===========================================================

Inputs:
1. factor_members.csv  -> columns = factor legs, rows = tickers
2. leverage_scores.csv -> columns = same factor legs, rows = leverage scores

Outputs:
- Cleaned and deduplicated factors
- Top & bottom leverage factors
- Overlap heatmap between factor legs
- Top leveraged companies
- Company co-occurrence (which firms often appear together)
===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === Config ===
WORKDIR = os.getcwd()
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

TOP_N_COMPANIES = 25
TOP_N_LEGS = 15
TOP_N_PAIRS = 15  # for company co-occurrence

# === Load ===
members = pd.read_csv(MEMBERS_CSV, dtype=str)
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str)

# === Clean column names ===
def clean_name(name):
    name = str(name).strip().upper()
    name = name.replace(" ", "_")
    name = name.replace("-", "_")
    name = name.split("_1")[0]  # remove "_1" or similar suffix
    name = name.split("1")[0] if name.endswith("1") else name  # remove trailing 1s
    name = ''.join(ch for ch in name if ch.isalnum() or ch == "_")
    return name

members.columns = [clean_name(c) for c in members.columns]
leverage.columns = [clean_name(c) for c in leverage.columns]

# Remove exact duplicate columns (keep first occurrence)
members = members.loc[:, ~members.columns.duplicated()]
leverage = leverage.loc[:, ~leverage.columns.duplicated()]

# Keep only columns that exist in both
common_cols = sorted(set(members.columns) & set(leverage.columns))
members = members[common_cols].applymap(lambda x: x.strip().upper() if isinstance(x, str) else x)
leverage = leverage[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)

# === Align row counts ===
max_rows = max(members.shape[0], leverage.shape[0])
members = members.reindex(range(max_rows))
leverage = leverage.reindex(range(max_rows))

# === Stack and combine ===
members_stack = members.stack().reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack().reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Score"]

df = pd.merge(members_stack, leverage_stack, on=["row_idx", "Factor_Leg"], how="left")

df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()
df["Leverage_Score"] = pd.to_numeric(df["Leverage_Score"].astype(str).str.replace(",", ""), errors="coerce")
df = df.dropna(subset=["Ticker", "Leverage_Score"])
df = df.drop_duplicates(subset=["Ticker", "Factor_Leg"])  # remove duplicates

# === Compute summaries ===
ticker_group = (
    df.groupby("Ticker")
    .agg(
        Max_Leverage=("Leverage_Score", "max"),
        Mean_Leverage=("Leverage_Score", "mean"),
        Count_Legs=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Max_Leverage", ascending=False)
)

leg_stats = (
    df.groupby("Factor_Leg")
    .agg(
        Avg_Leverage=("Leverage_Score", "mean"),
        Median_Leverage=("Leverage_Score", "median"),
        Count_Tickers=("Ticker", "nunique")
    )
    .reset_index()
)

leg_stats = leg_stats.sort_values("Avg_Leverage", ascending=False).reset_index(drop=True)

# === Factor Overlap Matrix ===
legs = sorted(df["Factor_Leg"].unique())
leg_sets = {leg: set(df.loc[df["Factor_Leg"] == leg, "Ticker"]) for leg in legs}

overlap = pd.DataFrame(index=legs, columns=legs, dtype=int)
for a in legs:
    for b in legs:
        if a != b:
            overlap.loc[a, b] = len(leg_sets[a].intersection(leg_sets[b]))
        else:
            overlap.loc[a, b] = np.nan

# Focus on top overlapping legs
top_legs = overlap.sum(axis=1).sort_values(ascending=False).head(TOP_N_LEGS).index.tolist()
overlap_top = overlap.loc[top_legs, top_legs]

# === Company Co-Occurrence Matrix ===
company_factors = df.groupby("Ticker")["Factor_Leg"].apply(set)
companies = company_factors.index.tolist()
co_matrix = pd.DataFrame(index=companies, columns=companies, dtype=int)

for i, c1 in enumerate(companies):
    for j, c2 in enumerate(companies):
        if i < j:
            overlap_count = len(company_factors[c1].intersection(company_factors[c2]))
            co_matrix.loc[c1, c2] = overlap_count
            co_matrix.loc[c2, c1] = overlap_count

# Top co-occurring pairs
co_pairs = []
for i, c1 in enumerate(companies):
    for j, c2 in enumerate(companies):
        if i < j and pd.notna(co_matrix.loc[c1, c2]) and co_matrix.loc[c1, c2] > 0:
            co_pairs.append((c1, c2, co_matrix.loc[c1, c2]))
co_df = pd.DataFrame(co_pairs, columns=["Company1", "Company2", "Shared_Factors"])
co_df = co_df.sort_values("Shared_Factors", ascending=False).head(TOP_N_PAIRS)

# === Visualisation 1: Top & Bottom Leverage Factors ===
top_legs_df = leg_stats.head(10)
bottom_legs_df = leg_stats.tail(10).sort_values("Avg_Leverage")

fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)
axes[0].barh(top_legs_df["Factor_Leg"], top_legs_df["Avg_Leverage"], color="tomato", edgecolor="k")
axes[0].set_title("Top 10 Factors by Average Leverage")
axes[0].invert_yaxis()
axes[0].set_xlabel("Average Leverage Score")

axes[1].barh(bottom_legs_df["Factor_Leg"], bottom_legs_df["Avg_Leverage"], color="lightblue", edgecolor="k")
axes[1].set_title("Bottom 10 Factors by Average Leverage")
axes[1].set_xlabel("Average Leverage Score")

plt.tight_layout()
plt.show()

# === Visualisation 2: Factor Overlap Heatmap ===
plt.figure(figsize=(8, 6))
plt.imshow(overlap_top.values, cmap="Blues", interpolation="nearest")
plt.colorbar(label="Shared Companies")
plt.xticks(range(len(top_legs)), top_legs, rotation=90)
plt.yticks(range(len(top_legs)), top_legs)
plt.title(f"Factor Overlap Matrix (Top {TOP_N_LEGS})")
plt.tight_layout()
plt.show()

# === Visualisation 3: Top Leveraged Companies ===
top_companies = ticker_group.head(TOP_N_COMPANIES)
plt.figure(figsize=(10, max(5, 0.3 * TOP_N_COMPANIES)))
plt.barh(top_companies["Ticker"], top_companies["Max_Leverage"], color="crimson", edgecolor="k")
plt.xlabel("Max Leverage Score")
plt.title(f"Top {TOP_N_COMPANIES} Most Leveraged Companies")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# === Visualisation 4: Company Co-Occurrence Heatmap ===
if not co_df.empty:
    plt.figure(figsize=(10, 6))
    plt.barh(
        [f"{a} & {b}" for a, b in zip(co_df["Company1"], co_df["Company2"])],
        co_df["Shared_Factors"],
        color="gray",
        edgecolor="k"
    )
    plt.xlabel("Number of Shared Factors")
    plt.title("Top Company Pairs Appearing Together in Multiple Factors")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

# === Summary Tables ===
print("\nTop 10 Highest-Leverage Factor Legs:\n")
print(leg_stats.head(10).to_string(index=False, justify="center", formatters={"Avg_Leverage": "{:.2f}".format}))

print("\nBottom 10 Lowest-Leverage Factor Legs:\n")
print(leg_stats.tail(10).to_string(index=False, justify="center", formatters={"Avg_Leverage": "{:.2f}".format}))

print("\nTop 10 Standout Leveraged Companies:\n")
print(ticker_group.head(10).to_string(index=False))

print("\nTop 15 Company Pairs Appearing in the Same Factors:\n")
print(co_df.to_string(index=False))

"""
===========================================================
  Factor Leg Leverage & Overlap Analysis (Final Refined)
===========================================================

Purpose:
- Takes two Bloomberg-style CSVs:
    1. factor_members.csv  -> columns = factor legs, rows = tickers
    2. leverage_scores.csv -> columns = same factor legs, rows = leverage scores
- Aligns them positionally and cleans duplicates
- Outputs:
    • Average & median leverage per factor leg (top + bottom)
    • Most leveraged companies
    • Least leveraged factor legs
    • Clean overlap heatmap (top 15 overlapping)
    • Bar charts for top & bottom leverage factors
===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === Config ===
WORKDIR = os.getcwd()
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

TOP_N_COMPANIES = 25
TOP_N_LEGS = 15

# === Load ===
members = pd.read_csv(MEMBERS_CSV, dtype=str)
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str)

# === Clean columns ===
members.columns = members.columns.str.strip().str.upper().str.replace(r"[^A-Z0-9_]+", "", regex=True)
leverage.columns = leverage.columns.str.strip().str.upper().str.replace(r"[^A-Z0-9_]+", "", regex=True)

# Match common columns and ensure unique column names (remove duplicates like INDEX_1)
members = members.loc[:, ~members.columns.duplicated()]
leverage = leverage.loc[:, ~leverage.columns.duplicated()]

common_cols = sorted(set(members.columns) & set(leverage.columns))
members = members[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)
leverage = leverage[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Align row counts
max_rows = max(members.shape[0], leverage.shape[0])
members = members.reindex(range(max_rows))
leverage = leverage.reindex(range(max_rows))

# === Stack and merge ===
members_stack = members.stack().reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack().reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Score"]

df = pd.merge(members_stack, leverage_stack, on=["row_idx", "Factor_Leg"], how="left")

df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()
df["Leverage_Score"] = pd.to_numeric(
    df["Leverage_Score"].astype(str).str.replace(",", ""), errors="coerce"
)
df = df.dropna(subset=["Ticker", "Leverage_Score"])
df["Factor_Leg"] = df["Factor_Leg"].str.strip().str.upper().str.replace(r"[^A-Z0-9_]+", "", regex=True)

# === Compute summaries ===
ticker_group = (
    df.groupby("Ticker")
    .agg(
        Max_Leverage=("Leverage_Score", "max"),
        Mean_Leverage=("Leverage_Score", "mean"),
        Count_Legs=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Max_Leverage", ascending=False)
)

leg_stats = (
    df.groupby("Factor_Leg")
    .agg(
        Avg_Leverage=("Leverage_Score", "mean"),
        Median_Leverage=("Leverage_Score", "median"),
        Count_Tickers=("Ticker", "nunique")
    )
    .reset_index()
    .drop_duplicates(subset=["Factor_Leg"])
)

leg_stats = leg_stats.sort_values("Avg_Leverage", ascending=False).reset_index(drop=True)

# === Overlap Matrix ===
legs = sorted(df["Factor_Leg"].unique())
leg_sets = {leg: set(df.loc[df["Factor_Leg"] == leg, "Ticker"]) for leg in legs}

overlap = pd.DataFrame(index=legs, columns=legs, dtype=int)
for a in legs:
    for b in legs:
        overlap.loc[a, b] = len(leg_sets[a].intersection(leg_sets[b])) if a != b else np.nan

# Focus on top overlapping legs only
top_legs = (
    overlap.sum(axis=1)
    .sort_values(ascending=False)
    .head(TOP_N_LEGS)
    .index.tolist()
)
overlap_top = overlap.loc[top_legs, top_legs]

# === Visualisation 1: Top & Bottom Average Leverage Factors ===
top_legs_df = leg_stats.head(10)
bottom_legs_df = leg_stats.tail(10).sort_values("Avg_Leverage")

fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)

axes[0].barh(top_legs_df["Factor_Leg"], top_legs_df["Avg_Leverage"], color="tomato", edgecolor="k")
axes[0].set_title("Top 10 Factors by Average Leverage")
axes[0].invert_yaxis()
axes[0].set_xlabel("Average Leverage Score")

axes[1].barh(bottom_legs_df["Factor_Leg"], bottom_legs_df["Avg_Leverage"], color="lightblue", edgecolor="k")
axes[1].set_title("Bottom 10 Factors by Average Leverage")
axes[1].set_xlabel("Average Leverage Score")

plt.tight_layout()
plt.show()

# === Visualisation 2: Overlap Heatmap ===
plt.figure(figsize=(8, 6))
plt.imshow(overlap_top.values, cmap="Blues", interpolation="nearest")
plt.colorbar(label="Shared Companies")
plt.xticks(range(len(top_legs)), top_legs, rotation=90)
plt.yticks(range(len(top_legs)), top_legs)
plt.title(f"Factor Overlap Matrix (Top {TOP_N_LEGS} Most Overlapping)")
plt.tight_layout()
plt.show()

# === Visualisation 3: Top Leveraged Companies ===
top_companies = ticker_group.head(TOP_N_COMPANIES)
plt.figure(figsize=(10, max(5, 0.3 * TOP_N_COMPANIES)))
plt.barh(top_companies["Ticker"], top_companies["Max_Leverage"], color="crimson", edgecolor="k")
plt.xlabel("Max Leverage Score")
plt.title(f"Top {TOP_N_COMPANIES} Most Leveraged Companies")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# === Summary Tables ===
print("\nTop 10 Highest-Leverage Factor Legs:\n")
print(leg_stats.head(10).to_string(index=False, justify="center", formatters={"Avg_Leverage":"{:.2f}".format}))

print("\nBottom 10 Lowest-Leverage Factor Legs:\n")
print(leg_stats.tail(10).to_string(index=False, justify="center", formatters={"Avg_Leverage":"{:.2f}".format}))

print("\nTop 10 Standout Leveraged Companies:\n")
print(ticker_group.head(10).to_string(index=False))

"""
===========================================================
  Factor Leg Leverage & Overlap Analysis (Refined Version)
===========================================================

CHANGES & IMPROVEMENTS
✅ Deduplicates factor leg names (case-insensitive, trims whitespace)
✅ Removes self-pairs & overpopulated parts of the overlap heatmap (only top N legs)
✅ Cleans duplicate entries in the per-leg leverage summary
✅ Adds a clear section highlighting standout individual companies with bar chart
===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import networkx as nx

# === Config ===
WORKDIR = os.getcwd()
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

TOP_N_COMPANIES = 25       # top companies to display clearly
TOP_N_LEGS = 15            # how many top legs to show in overlap heatmap

# === Load ===
members = pd.read_csv(MEMBERS_CSV, dtype=str)
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str)

# === Clean columns ===
members.columns = members.columns.str.strip().str.upper()
leverage.columns = leverage.columns.str.strip().str.upper()

common_cols = sorted(set(members.columns) & set(leverage.columns))
members = members[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)
leverage = leverage[common_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Align rows
max_rows = max(members.shape[0], leverage.shape[0])
members = members.reindex(range(max_rows))
leverage = leverage.reindex(range(max_rows))

# === Stack to long format ===
members_stack = members.stack().reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack().reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Score"]

df = pd.merge(members_stack, leverage_stack, on=["row_idx", "Factor_Leg"], how="left")
df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()
df["Leverage_Score"] = pd.to_numeric(df["Leverage_Score"].astype(str).str.replace(",", ""), errors="coerce")
df = df.dropna(subset=["Ticker", "Leverage_Score"])

# === Compute metrics ===
# Clean duplicate factors again after merge
df["Factor_Leg"] = df["Factor_Leg"].str.strip().str.upper()

ticker_group = (
    df.groupby("Ticker")
    .agg(
        Max_Leverage=("Leverage_Score", "max"),
        Mean_Leverage=("Leverage_Score", "mean"),
        Count_Legs=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Max_Leverage", ascending=False)
)

leg_stats = (
    df.groupby("Factor_Leg")
    .agg(
        Avg_Leverage=("Leverage_Score", "mean"),
        Median_Leverage=("Leverage_Score", "median"),
        Count_Tickers=("Ticker", "nunique")
    )
    .reset_index()
    .drop_duplicates(subset=["Factor_Leg"])
    .sort_values("Avg_Leverage", ascending=False)
)

# === Overlap matrix (cleaner version) ===
legs = sorted(df["Factor_Leg"].unique())
leg_sets = {leg: set(df.loc[df["Factor_Leg"] == leg, "Ticker"]) for leg in legs}

overlap = pd.DataFrame(index=legs, columns=legs, dtype=int)
for a in legs:
    for b in legs:
        if a == b:
            overlap.loc[a, b] = np.nan  # remove diagonal
        else:
            overlap.loc[a, b] = len(leg_sets[a].intersection(leg_sets[b]))

# Only show top N most overlapping legs
top_legs = (
    overlap.sum(axis=1)
    .sort_values(ascending=False)
    .head(TOP_N_LEGS)
    .index.tolist()
)
overlap_top = overlap.loc[top_legs, top_legs]

# === Visualization 1: average leverage per factor leg ===
plt.figure(figsize=(10, max(5, 0.4 * len(leg_stats.head(20)))))
plt.barh(leg_stats["Factor_Leg"].head(20), leg_stats["Avg_Leverage"].head(20), color='orange', edgecolor='k')
plt.xlabel("Average Leverage Score")
plt.title("Top 20 Factor Legs by Average Leverage")
plt.gca().invert_yaxis()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# === Visualization 2: overlap heatmap ===
plt.figure(figsize=(8, 6))
plt.imshow(overlap_top.values, cmap="Blues", interpolation="nearest")
plt.colorbar(label="Shared Companies")
plt.xticks(range(len(top_legs)), top_legs, rotation=90)
plt.yticks(range(len(top_legs)), top_legs)
plt.title(f"Factor Overlap Matrix (Top {TOP_N_LEGS} Most Overlapping)")
plt.tight_layout()
plt.show()

# === Visualization 3: standout leveraged companies ===
top_companies = ticker_group.head(TOP_N_COMPANIES)
plt.figure(figsize=(10, max(5, 0.3 * TOP_N_COMPANIES)))
plt.barh(top_companies["Ticker"], top_companies["Max_Leverage"], color='crimson', edgecolor='k')
plt.xlabel("Max Leverage Score")
plt.title(f"Top {TOP_N_COMPANIES} Most Leveraged Companies")
plt.gca().invert_yaxis()
for i, v in enumerate(top_companies["Max_Leverage"]):
    plt.text(v + 0.5, i, f"{v:.1f}", va="center", fontsize=8)
plt.tight_layout()
plt.show()

# === Optional: Show overlap count for top names ===
print("\n=== Standout Leveraged Companies ===")
print(ticker_group.head(10).to_string(index=False))
print("\nEach of these appears in multiple factor legs, often driving overlap or high average leverage.")

"""
===========================================================
  Factor Leg Leverage & Overlap Analysis (robust, improved)
===========================================================

WHAT THIS DOES (high level)
- Reads two CSVs with the same *column layout*:
    1) factor_members.csv  -> each column is a factor leg; rows list tickers
    2) leverage_scores.csv -> same shape; rows contain leverage score for the ticker at same cell
- Pairs each ticker with its leverage score using positional alignment (row + column).
- Produces:
    • A ranked list of companies by leverage (overall & per-leg)
    • Which factor legs each company belongs to (overlap mapping)
    • An overlap matrix (how many tickers shared between legs)
    • Visualizations:
        - Average leverage by leg (bar chart)
        - Distribution of leverage (histogram)
        - Overlap heatmap (legs × legs)
        - Network graph (factors <-> top leveraged companies)
- Saves results to CSV files for downstream review.

USAGE
- Put this script in the same folder as:
    ./factor_members.csv
    ./leverage_scores.csv
- Then run in Jupyter. If networkx is not installed, uncomment the pip install line.

NOTES & ASSUMPTIONS
- Both CSVs must have identical column names (factor leg names) and the same number of *rows*.
- Empty cells are treated as missing.
- Tickers are treated as strings and matched only by position (same row/column).
  If your tickers appear in different order between files, rearrange so they align.
===========================================================
"""

# ---------- Imports ----------
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# Optional: network graph
try:
    import networkx as nx
except Exception:
    # uncomment next line to install if needed in notebook
    # !pip install networkx
    import networkx as nx

# ---------- Config ----------
WORKDIR = os.getcwd()                      # or set explicit path
MEMBERS_CSV = os.path.join(WORKDIR, "factor_members.csv")
LEVERAGE_CSV = os.path.join(WORKDIR, "leverage_scores.csv")

OUT_SUMMARY = os.path.join(WORKDIR, "factor_leg_leverage_summary.csv")
OUT_TOP = os.path.join(WORKDIR, "top_leveraged_companies.csv")
OUT_OVERLAP_MATRIX = os.path.join(WORKDIR, "factor_overlap_matrix.csv")
TOP_N_COMPANIES = 50   # number of top companies to show/save/network

# ---------- Load & basic validation ----------
if not os.path.exists(MEMBERS_CSV):
    raise FileNotFoundError(f"Missing file: {MEMBERS_CSV}")
if not os.path.exists(LEVERAGE_CSV):
    raise FileNotFoundError(f"Missing file: {LEVERAGE_CSV}")

members = pd.read_csv(MEMBERS_CSV, dtype=str, keep_default_na=False, na_values=["", "NA", "NaN"])
leverage = pd.read_csv(LEVERAGE_CSV, dtype=str, keep_default_na=False, na_values=["", "NA", "NaN"])

# Trim whitespace from column names & values
members.columns = [c.strip() for c in members.columns]
leverage.columns = [c.strip() for c in leverage.columns]
members = members.applymap(lambda x: x.strip() if isinstance(x, str) else x)
leverage = leverage.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Basic shape check: if number of columns differ or column names differ, warn but proceed using intersection
common_cols = [c for c in members.columns if c in leverage.columns]
if len(common_cols) != len(members.columns) or len(common_cols) != len(leverage.columns):
    print("⚠️ Column mismatch detected. Using intersection of columns:")
    print(" - Members columns:", members.columns.tolist())
    print(" - Leverage columns:", leverage.columns.tolist())
    print(" - Using common columns:", common_cols)
    # subset to common columns in same order
    members = members[common_cols].copy()
    leverage = leverage[common_cols].copy()

# Ensure same number of rows: if different, pad shorter with NaN to align by position
if members.shape[0] != leverage.shape[0]:
    max_rows = max(members.shape[0], leverage.shape[0])
    members = members.reindex(range(max_rows)).reset_index(drop=True)
    leverage = leverage.reindex(range(max_rows)).reset_index(drop=True)
    print(f"Info: rows differed and were aligned to {max_rows} rows by position.")

# ---------- Stack both tables to long format while preserving row position ----------
# The stack method preserves (row_index, column_name) pairing
members_stack = members.stack(dropna=True).reset_index()
members_stack.columns = ["row_idx", "Factor_Leg", "Ticker"]

leverage_stack = leverage.stack(dropna=True).reset_index()
leverage_stack.columns = ["row_idx", "Factor_Leg", "Leverage_Score_raw"]

# Merge on both row_idx and Factor_Leg => the positional pairings
combined = pd.merge(
    members_stack,
    leverage_stack,
    on=["row_idx", "Factor_Leg"],
    how="left"
)

# Clean tickers: remove empty/placeholder tickers
combined["Ticker"] = combined["Ticker"].astype(str).str.strip()
combined = combined[combined["Ticker"].str.len() > 0].copy()

# Normalize ticker strings (optional: uppercase)
combined["Ticker_norm"] = combined["Ticker"].str.upper()

# Clean leverage scores: coerce to numeric
combined["Leverage_Score"] = pd.to_numeric(combined["Leverage_Score_raw"].astype(str).str.replace(",", "").str.replace("%", ""), errors="coerce")

# Drop rows where leverage is NaN (can't analyze)
na_count = combined["Leverage_Score"].isna().sum()
if na_count > 0:
    print(f"⚠️ {na_count} rows have missing/invalid leverage scores and will be ignored in numeric analysis.")

combined = combined.dropna(subset=["Leverage_Score"])

# ---------- Aggregate analysis ----------
# 1) For each ticker: get max leverage (if appears multiple times with different scores), and list of legs
ticker_group = (
    combined.groupby("Ticker_norm")
    .agg(
        Max_Leverage=("Leverage_Score", "max"),
        Mean_Leverage=("Leverage_Score", "mean"),
        Count_Appearances=("Factor_Leg", "nunique"),
        Legs=("Factor_Leg", lambda x: ", ".join(sorted(set(x))))
    )
    .reset_index()
    .sort_values("Max_Leverage", ascending=False)
)

# 2) For each factor leg: compute average leverage and top names
leg_stats = (
    combined.groupby("Factor_Leg")
    .agg(
        Avg_Leverage=("Leverage_Score", "mean"),
        Median_Leverage=("Leverage_Score", "median"),
        Count_Tickers=("Ticker_norm", "nunique")
    )
    .reset_index()
    .sort_values("Avg_Leverage", ascending=False)
)

# 3) Overlap matrix: for each pair of legs count common tickers
legs = sorted(combined["Factor_Leg"].unique())
overlap_matrix = pd.DataFrame(index=legs, columns=legs, dtype=int).fillna(0)
# prepare factor->set mapping
leg_to_set = {leg: set(combined.loc[combined["Factor_Leg"]==leg, "Ticker_norm"].unique()) for leg in legs}
for a in legs:
    for b in legs:
        overlap_matrix.loc[a, b] = len(leg_to_set[a].intersection(leg_to_set[b]))

# ---------- Output & saves ----------
ticker_group.head(TOP_N_COMPANIES := TOP_N_COMPANIES if 'TOP_N_COMPANIES' in globals() else 50)  # safety; variable defined earlier maybe
# Save summary CSVs
ticker_group.to_csv(OUT_TOP if 'OUT_TOP' in globals() else os.path.join(WORKDIR,"top_leveraged_companies.csv"), index=False)
leg_stats.to_csv(OUT_SUMMARY if 'OUT_SUMMARY' in globals() else os.path.join(WORKDIR,"factor_leg_leverage_summary.csv"), index=False)
overlap_matrix.to_csv(OUT_OVERLAP_MATRIX if 'OUT_OVERLAP_MATRIX' in globals() else os.path.join(WORKDIR,"factor_overlap_matrix.csv"))

print("\n✅ Saved outputs:")
print(" - Top companies (by leverage):", OUT_TOP if 'OUT_TOP' in globals() else os.path.join(WORKDIR,"top_leveraged_companies.csv"))
print(" - Per-leg leverage summary:", OUT_SUMMARY if 'OUT_SUMMARY' in globals() else os.path.join(WORKDIR,"factor_leg_leverage_summary.csv"))
print(" - Overlap matrix:", OUT_OVERLAP_MATRIX if 'OUT_OVERLAP_MATRIX' in globals() else os.path.join(WORKDIR,"factor_overlap_matrix.csv"))

# ---------- Display quick tables ----------
print("\n=== Top 20 companies by Max Leverage ===")
display(ticker_group.head(20).style.format({"Max_Leverage":"{:.2f}", "Mean_Leverage":"{:.2f}"}))

print("\n=== Factor Leg Average Leverage (top 20) ===")
display(leg_stats.head(20).style.format({"Avg_Leverage":"{:.2f}", "Median_Leverage":"{:.2f}"}))

# ---------- Visualizations ----------
plt.rcParams.update({'figure.max_open_warning': 0})

# 1) Bar chart: avg leverage per leg
plt.figure(figsize=(10, max(4, 0.25*len(leg_stats))))
plt.barh(leg_stats["Factor_Leg"], leg_stats["Avg_Leverage"], color='tab:orange', edgecolor='k')
plt.xlabel("Average Leverage Score")
plt.title("Average Leverage by Factor Leg")
plt.gca().invert_yaxis()
for i, v in enumerate(leg_stats["Avg_Leverage"]):
    plt.text(v + max(leg_stats["Avg_Leverage"])*0.01, i, f"{v:.2f}", va='center')
plt.tight_layout()
plt.show()

# 2) Overlap heatmap
plt.figure(figsize=(8, 6))
plt.imshow(overlap_matrix.values.astype(int), cmap="Blues", interpolation='nearest')
plt.colorbar(label="Number of shared tickers")
plt.xticks(range(len(legs)), legs, rotation=90)
plt.yticks(range(len(legs)), legs)
plt.title("Overlap Matrix: Shared Tickers Between Factor Legs")
# annotate
for i in range(len(legs)):
    for j in range(len(legs)):
        plt.text(j, i, str(overlap_matrix.iloc[i,j]), ha='center', va='center', color='black', fontsize=8)
plt.tight_layout()
plt.show()

# 3) Histogram of leverage scores across all tickers
plt.figure(figsize=(8,4))
plt.hist(combined["Leverage_Score"], bins=40, color='steelblue', edgecolor='k', alpha=0.8)
plt.xlabel("Leverage Score")
plt.ylabel("Count")
plt.title("Distribution of Leverage Scores (all factor legs)")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 4) Network graph (bipartite) for top leveraged companies and their legs
TOP_NET = min(40, len(ticker_group))  # limit nodes for readability
top_tickers = ticker_group.head(TOP_NET)["Ticker_norm"].tolist()
G = nx.Graph()
# add factor nodes and company nodes (prefix to avoid name clash)
for leg in legs:
    G.add_node(f"LEG::{leg}", bipartite=0, type='leg')
for t in top_tickers:
    G.add_node(f"COMP::{t}", bipartite=1, type='comp', leverage=float(ticker_group.loc[ticker_group["Ticker_norm"]==t, "Max_Leverage"].values[0]))

# add edges only where top companies occur
edges = []
for leg in legs:
    members_in_leg = leg_to_set[leg].intersection(set(top_tickers))
    for t in members_in_leg:
        edges.append((f"LEG::{leg}", f"COMP::{t}"))
G.add_edges_from(edges)

# layout and plot
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G, k=0.5, seed=42)  # layout
leg_nodes = [n for n, d in G.nodes(data=True) if d.get('type')=='leg']
comp_nodes = [n for n, d in G.nodes(data=True) if d.get('type')=='comp']
# node sizes: legs fixed, comps scaled by leverage
comp_sizes = [300 + 50*(G.nodes[n]['leverage']) for n in comp_nodes]
nx.draw_networkx_nodes(G, pos, nodelist=leg_nodes, node_shape='s', node_color='lightgreen', node_size=700, label='Legs')
nx.draw_networkx_nodes(G, pos, nodelist=comp_nodes, node_shape='o', node_color='salmon', node_size=comp_sizes, label='Companies')
nx.draw_networkx_edges(G, pos, alpha=0.6)
# labels: show leg label without prefix and company ticker
labels = {n: (n.split("::")[1]) for n in G.nodes()}
nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)
plt.title(f"Network: Top {len(top_tickers)} Leveraged Companies (size ~ leverage) and their Legs")
plt.axis('off')
plt.tight_layout()
plt.show()

# ---------- Done ----------
print("\n✅ Analysis complete. Outputs saved and charts displayed.")
print(" - If you want the full merged long table, use the 'combined' DataFrame (rows = ticker occurrence per leg).")

"""
===========================================================
        Linking Factor Credit Sensitivity to Leverage
===========================================================

📊 GOAL:
See if factors with higher credit betas (from your factor_credit_results.csv)
also have higher average leverage exposure in their underlying names.

===========================================================

📂 REQUIRED FILES
Place this notebook and these CSVs in the same folder:
   Credit_Leverage_Link/
   ├── Credit_Leverage_Link.ipynb
   ├── factor_credit_results.csv      # from your previous regression script
   ├── factor_members.csv             # factor basket constituents (see below)
   └── leverage_scores.csv            # Bloomberg leverage data

===========================================================

📄 factor_members.csv
-----------------------------------------------------------
| Factor_Name | Ticker | Weight |
|--------------|---------|--------|
| VALUE_FAC    | AAPL US Equity | 0.025 |
| VALUE_FAC    | MSFT US Equity | 0.030 |
| MOMENTUM_FAC | META US Equity | 0.028 |
| MOMENTUM_FAC | TSLA US Equity | 0.035 |

✅ Export from Bloomberg:
   - Use EQS <GO> or MEMB <GO> on each basket.
   - Include Ticker, Weight, and the factor name (can be typed manually).

===========================================================

📄 leverage_scores.csv
-----------------------------------------------------------
| Ticker | Company_Name | FA_LEVERAGE_SCORE |
|---------|---------------|------------------|
| AAPL US Equity | Apple Inc | 32.5 |
| MSFT US Equity | Microsoft Corp | 28.1 |
| META US Equity | Meta Platforms | 61.7 |
| TSLA US Equity | Tesla Inc | 73.2 |

✅ Export from Bloomberg Excel Add-In:
   Use =BDP("AAPL US Equity","FA_LEVERAGE_SCORE")
   Or for batch =BDP(A2:A100,"FA_LEVERAGE_SCORE")

===========================================================
"""

import pandas as pd
import os
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.stats import linregress

# === Load files ===
folder = os.path.dirname(os.path.abspath(__file__)) if "__file__" in globals() else os.getcwd()

factor_credit_path = os.path.join(folder, "factor_credit_results.csv")
members_path = os.path.join(folder, "factor_members.csv")
leverage_path = os.path.join(folder, "leverage_scores.csv")

factor_credit = pd.read_csv(factor_credit_path)
members = pd.read_csv(members_path)
leverage = pd.read_csv(leverage_path)

# Clean up columns
for df in [factor_credit, members, leverage]:
    df.columns = df.columns.str.strip()

# === Compute average leverage per factor ===
merged = members.merge(leverage, on="Ticker", how="left")
merged["Weighted_Leverage"] = merged["Weight"] * merged["FA_LEVERAGE_SCORE"]

factor_leverage = (
    merged.groupby("Factor_Name")
    .apply(lambda x: (x["Weighted_Leverage"].sum() / x["Weight"].sum()))
    .reset_index(name="Weighted_Avg_Leverage")
)

print("✅ Computed weighted leverage per factor basket:\n")
print(factor_leverage.round(2))

# === Merge leverage info with credit beta results ===
# Keep only iTraxx Main betas for simplicity
main_betas = factor_credit[factor_credit["Credit_Spread"] == "ITRX_MAIN"].copy()
main_betas.rename(columns={"Beta": "Credit_Beta"}, inplace=True)

merged_factors = main_betas.merge(factor_leverage, left_on="Factor", right_on="Factor_Name", how="left")

# === Clean and prepare ===
merged_factors = merged_factors[["Factor", "Credit_Beta", "R²", "Corr", "Weighted_Avg_Leverage"]].dropna()

# === Check correlation between leverage and beta ===
corr = merged_factors["Credit_Beta"].corr(merged_factors["Weighted_Avg_Leverage"])
print(f"\n📈 Correlation between leverage exposure and credit beta: {corr:.2f}")

# === Simple regression: does leverage explain credit beta? ===
X = sm.add_constant(merged_factors["Weighted_Avg_Leverage"])
y = merged_factors["Credit_Beta"]
model = sm.OLS(y, X).fit()
print("\n=== Regression: Credit Beta ~ Leverage Exposure ===")
print(model.summary())

# === Visualize relationship ===
plt.figure(figsize=(7,5))
plt.scatter(merged_factors["Weighted_Avg_Leverage"], merged_factors["Credit_Beta"], alpha=0.7, color="steelblue")
slope, intercept, r, p, _ = linregress(merged_factors["Weighted_Avg_Leverage"], merged_factors["Credit_Beta"])
plt.plot(merged_factors["Weighted_Avg_Leverage"], intercept + slope * merged_factors["Weighted_Avg_Leverage"],
         color="crimson", linewidth=2, label=f"Fit: β={slope:.2f}, R²={r**2:.2f}")
plt.xlabel("Weighted Avg Leverage Score (Factor Basket)")
plt.ylabel("Credit Beta (iTraxx Main)")
plt.title("Factor Credit Sensitivity vs Leverage Exposure")
plt.legend()
plt.grid(alpha=0.6, linestyle="--")
plt.tight_layout()
plt.show()

# === Highlight top 5 leveraged and credit-sensitive factors ===
top_linked = merged_factors.sort_values("Credit_Beta", ascending=False).head(5)
print("\n=== 🔝 Top 5 Most Credit-Sensitive (and Levered) Factors ===")
print(top_linked.round(3))

# === Save summary ===
output_file = os.path.join(folder, "credit_leverage_link.csv")
merged_factors.to_csv(output_file, index=False)
print(f"\n✅ Saved merged factor leverage and credit sensitivity summary to: {output_file}")

"""
===========================================================
  Factor vs Credit Spread Sensitivity Analysis (robust)
===========================================================

Copy & paste into Jupyter. This script:
 - Handles ambiguous date formats (tries dayfirst False/True)
 - Cleans numeric columns (removes commas, $)
 - Computes daily returns
 - Runs regressions (OLS) of each factor vs ITRX_MAIN and ITRX_XOVER
 - Saves results to factor_credit_results.csv
 - Plots:
     * Heatmap of betas
     * Ranked bar chart of most sensitive factors
     * Scatter + regression lines for top factors

INSTRUCTIONS:
 - Place this notebook and your CSV factors_credit.csv in same folder.
 - CSV format (simple): first column "Date", other columns are factor/index levels,
   and must include columns named "ITRX_MAIN" and "ITRX_XOVER" (or similar - see detection).
 - Date may be in dd/mm/YYYY or mm/dd/YYYY; the script will try to auto-detect.

If parsing still fails, inspect the Date column values and standardize them in Excel.

===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy import stats

# ---------------------------
# Configuration / filenames
# ---------------------------
FILE = "factors_credit.csv"   # your CSV
OUT_RESULTS = "factor_credit_results.csv"
TOP_N = 8   # how many top factors to show in ranked chart

# ---------------------------
# Helper: robust date parse
# ---------------------------
def parse_dates_robust(series):
    """
    Try parsing the Date series with dayfirst=False and dayfirst=True.
    Return the parsed Series and which mode was chosen.
    """
    s = series.astype(str).str.strip()
    parsed_false = pd.to_datetime(s, dayfirst=False, infer_datetime_format=True, errors='coerce')
    parsed_true  = pd.to_datetime(s, dayfirst=True,  infer_datetime_format=True, errors='coerce')
    count_false = parsed_false.notna().sum()
    count_true  = parsed_true.notna().sum()
    # Choose the parser that yields more valid dates (or default to False if tie)
    if count_true > count_false:
        chosen = parsed_true
        method = "dayfirst=True"
    else:
        chosen = parsed_false
        method = "dayfirst=False"
    # If very few parsed successfully, return chosen but warn
    if chosen.notna().sum() / len(chosen) < 0.5:
        print("⚠️ Warning: less than 50% of Date values were parsed successfully.")
        print("Sample Date values (first 10):")
        print(series.head(10).to_list())
        print("Attempting with chosen method:", method)
    return chosen, method

# ---------------------------
# Load file
# ---------------------------
if not os.path.exists(FILE):
    raise FileNotFoundError(f"Could not find '{FILE}' in the working folder ({os.getcwd()}).\n"
                            "Ensure the CSV is named exactly and in the same folder as this notebook.")

print(f"✅ Loading file: {FILE}")
raw = pd.read_csv(FILE, low_memory=False)

# Find date column (case-insensitive match)
date_cols = [c for c in raw.columns if 'date' in c.lower()]
if not date_cols:
    # fallback: assume first column is date
    date_col = raw.columns[0]
    print(f"⚠️ No column containing 'date' found — using first column as Date: '{date_col}'")
else:
    date_col = date_cols[0]
    print(f"Using '{date_col}' as Date column")

# Parse dates robustly
parsed_dates, method_used = parse_dates_robust(raw[date_col])
raw[date_col] = parsed_dates
print(f"Date parsing method chosen: {method_used}")
print(f"Parsed dates: {parsed_dates.notna().sum()} / {len(parsed_dates)} valid")

# If many NaT remain, try fallback: strip time components or replace common separators
if raw[date_col].isna().sum() > 0:
    # try cleaning common formats (replace '.' with '/')
    sample = raw[date_col].astype(str).head(20).tolist()
    # attempt one more robust try by replacing '.' -> '/'
    alt = raw[date_col].astype(str).str.replace('.', '/', regex=False)
    alt_parsed = pd.to_datetime(alt, dayfirst=True, infer_datetime_format=True, errors='coerce')
    if alt_parsed.notna().sum() > parsed_dates.notna().sum():
        raw[date_col] = alt_parsed
        print("Applied alternate parsing (replaced '.' with '/'). Improved parsing count:",
              alt_parsed.notna().sum())

# Final check
if raw[date_col].isna().all():
    raise ValueError("Unable to parse any Date values. Please ensure Date column is in a standard format "
                     "(e.g., YYYY-MM-DD, DD/MM/YYYY or MM/DD/YYYY).")
elif raw[date_col].isna().sum() > 0:
    print(f"⚠️ Note: {raw[date_col].isna().sum()} rows have unparsed Date values and will be dropped.")

# ---------------------------
# Convert Date to index and sort
# ---------------------------
raw = raw.rename(columns={date_col: "Date"})
raw["Date"] = pd.to_datetime(raw["Date"])
raw = raw.sort_values("Date").reset_index(drop=True)
raw = raw.dropna(subset=["Date"])   # drop rows where Date couldn't be parsed
raw.set_index("Date", inplace=True)

# ---------------------------
# Detect credit spread columns (flexible)
# ---------------------------
cols_lower = [c.lower() for c in raw.columns]
credit_candidates = [c for c in raw.columns if any(k in c.lower() for k in ['itrax','itrx','iboxeig','iboxhyd','xover','crossover'])]
# Prefer explicit names if present
if "ITRX_MAIN" in raw.columns and "ITRX_XOVER" in raw.columns:
    credit_cols = ["ITRX_MAIN", "ITRX_XOVER"]
elif len(credit_candidates) >= 2:
    credit_cols = credit_candidates[:2]
    print("Detected credit columns:", credit_cols)
else:
    # Last resort: require user to have columns named ITRX_MAIN and ITRX_XOVER
    if "ITRX_MAIN" in raw.columns or "ITRX_XOVER" in raw.columns:
        credit_cols = [c for c in ["ITRX_MAIN","ITRX_XOVER"] if c in raw.columns]
        print("Using found credit columns:", credit_cols)
    else:
        raise ValueError("Could not detect credit spread columns. Please ensure your CSV contains columns named "
                         "'ITRX_MAIN' and 'ITRX_XOVER', or columns containing 'itrax' / 'iboxeig' / 'iboxhyd' / 'xover'.")

print("Credit columns used:", credit_cols)

# ---------------------------
# Clean numeric columns (remove commas, $) and coerce to float
# ---------------------------
def clean_numeric_column(s):
    return pd.to_numeric(s.astype(str).str.replace(',', '', regex=False)
                           .str.replace('$', '', regex=False)
                           .str.replace('%', '', regex=False)
                           .replace('nan', np.nan), errors='coerce')

# Apply cleaning to all non-date columns
for col in raw.columns:
    raw[col] = clean_numeric_column(raw[col])

# Drop columns that are completely NaN after cleaning (optional)
all_nan_cols = [c for c in raw.columns if raw[c].isna().all()]
if all_nan_cols:
    print("⚠️ Dropping columns that are all NaN after numeric cleaning:", all_nan_cols)
    raw = raw.drop(columns=all_nan_cols)

# ---------------------------
# Compute daily returns
# ---------------------------
returns = raw.pct_change().dropna()
print(f"Computed returns: {returns.shape[0]} rows × {returns.shape[1]} columns")

# ---------------------------
# Identify factor columns
# ---------------------------
factors = [c for c in returns.columns if c not in credit_cols]
print("Found factor columns:", factors)

# ---------------------------
# Regression function
# ---------------------------
import statsmodels.api as sm

def run_regression(y, x):
    # remove NaNs
    mask = y.notna() & x.notna()
    if mask.sum() < 10:
        return {"Beta": np.nan, "Alpha": np.nan, "R2": np.nan, "t": np.nan, "p": np.nan, "Corr": np.nan}
    X = sm.add_constant(x[mask])
    model = sm.OLS(y[mask], X).fit()
    return {
        "Beta": float(model.params[1]),
        "Alpha": float(model.params[0]),
        "R2": float(model.rsquared),
        "t": float(model.tvalues[1]) if len(model.tvalues)>1 else np.nan,
        "p": float(model.pvalues[1]) if len(model.pvalues)>1 else np.nan,
        "Corr": float(np.corrcoef(y[mask], x[mask])[0,1])
    }

# ---------------------------
# Run regressions for each factor vs each credit spread
# ---------------------------
rows = []
for factor in factors:
    for credit in credit_cols:
        stats_res = run_regression(returns[factor], returns[credit])
        row = {
            "Factor": factor,
            "Credit": credit,
            "Beta": stats_res["Beta"],
            "Alpha": stats_res["Alpha"],
            "R2": stats_res["R2"],
            "t-stat": stats_res["t"],
            "p-value": stats_res["p"],
            "Corr": stats_res["Corr"],
            "Abs_Beta": abs(stats_res["Beta"]) if not np.isnan(stats_res["Beta"]) else np.nan
        }
        rows.append(row)

results_df = pd.DataFrame(rows).sort_values(["Credit","Abs_Beta"], ascending=[True, False])
results_df.to_csv(OUT_RESULTS, index=False)
print(f"✅ Regression results saved to: {OUT_RESULTS}")

# ---------------------------
# Visual 1: Heatmap of Betas (Factors × Credit)
# ---------------------------
# pivot to matrix
beta_pivot = results_df.pivot(index="Factor", columns="Credit", values="Beta").fillna(0)

plt.figure(figsize=(10, max(4, 0.25*len(beta_pivot))))
plt.imshow(beta_pivot, aspect='auto', cmap='coolwarm', vmin=-np.nanmax(np.abs(beta_pivot.values)), vmax=np.nanmax(np.abs(beta_pivot.values)))
plt.colorbar(label="Beta")
plt.yticks(ticks=np.arange(len(beta_pivot.index)), labels=beta_pivot.index)
plt.xticks(ticks=np.arange(len(beta_pivot.columns)), labels=beta_pivot.columns)
plt.title("Heatmap of Betas (Factor sensitivity to Credit spreads)")
plt.tight_layout()
# annotate values
for i, row_idx in enumerate(beta_pivot.index):
    for j, col_idx in enumerate(beta_pivot.columns):
        val = beta_pivot.iloc[i, j]
        plt.text(j, i, f"{val:.2f}", ha="center", va="center", color="black", fontsize=8)
plt.show()

# ---------------------------
# Visual 2: Ranked bar chart of top factors by absolute beta (combined across credits)
# ---------------------------
# compute max absolute beta per factor across both credits
max_abs = results_df.groupby("Factor")["Abs_Beta"].max().sort_values(ascending=False)
top_factors = max_abs.head(TOP_N)
plt.figure(figsize=(10, 6))
bars = plt.barh(top_factors.index[::-1], top_factors.values[::-1], color='tab:blue')
plt.xlabel("Max |Beta| vs any credit spread")
plt.title(f"Top {TOP_N} Factors by maximum absolute beta to credit spreads")
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.gca().invert_yaxis()
for b in bars:
    plt.text(b.get_width() + 0.01, b.get_y() + b.get_height()/2, f"{b.get_width():.3f}", va='center')
plt.show()

# ---------------------------
# Visual 3: Scatter + regression lines for top 4 factors (by max abs beta)
# ---------------------------
top4 = max_abs.head(4).index.tolist()
for factor in top4:
    plt.figure(figsize=(12,5))
    for i, credit in enumerate(credit_cols):
        plt.subplot(1, len(credit_cols), i+1)
        x = returns[credit]
        y = returns[factor]
        mask = x.notna() & y.notna()
        slope, intercept, r_val, p_val, se = stats.linregress(x[mask], y[mask])
        plt.scatter(x[mask], y[mask], s=10, alpha=0.4)
        # plot regression line over scatter range
        xs = np.linspace(x[mask].min(), x[mask].max(), 100)
        plt.plot(xs, intercept + slope*xs, color='red', linewidth=2)
        plt.title(f"{factor} vs {credit}\nβ={slope:.3f}, R²={r_val**2:.3f}, p={p_val:.3g}")
        plt.xlabel(f"{credit} daily return")
        plt.ylabel(f"{factor} daily return")
        plt.grid(True, linestyle='--', alpha=0.4)
    plt.tight_layout()
    plt.show()

# ---------------------------
# Extra: print top table for quick review
# ---------------------------
print("\n=== Top factors (by max absolute beta) ===")
top_summary = (results_df.groupby("Factor")
               .agg(max_abs_beta=("Abs_Beta","max"),
                    best_credit=("Credit", lambda x: results_df.loc[ x.index[0] ,"Credit"] if False else None))
               .reset_index()
              )
# Simpler print using max_abs series
top_table = pd.DataFrame({
    "Factor": max_abs.head(TOP_N).index,
    "MaxAbsBeta": max_abs.head(TOP_N).values
})
print(top_table.to_string(index=False))

print("\n✅ Done — review the heatmap, ranked bars and scatter/regression plots above.")

"""
===========================================================
      Factor vs Credit Spread Sensitivity Analysis
===========================================================

INSTRUCTIONS:

1. Place this notebook and your Bloomberg CSV file in the same folder:
       Credit Analysis/
       ├── Credit_Analysis.ipynb
       └── factors_credit.csv

2. Your CSV must include daily levels with these columns:

   | Date | Factor_1 | Factor_2 | ... | ITRX_MAIN | ITRX_XOVER |

3. Save as: factors_credit.csv

4. This script will:
   • Compute daily returns
   • Run regressions vs iTraxx Main and XOver
   • Show correlation, beta, and R²
   • Plot relationships

===========================================================
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import statsmodels.api as sm

# === Load the Bloomberg CSV ===
folder = os.path.dirname(os.path.abspath(__file__)) if "__file__" in globals() else os.getcwd()
file_path = os.path.join(folder, "factors_credit.csv")

df = pd.read_csv(file_path)
df.columns = df.columns.str.strip()
df["Date"] = pd.to_datetime(df["Date"])
df.set_index("Date", inplace=True)
df = df.sort_index()

print(f"✅ Loaded data with {df.shape[0]} rows and {df.shape[1]} columns")

# === Compute daily percentage returns ===
returns = df.pct_change().dropna()

# === Identify factor columns (everything except the iTraxx columns) ===
itraxx_cols = ["ITRX_MAIN", "ITRX_XOVER"]
factor_cols = [c for c in returns.columns if c not in itraxx_cols]

print("\nAnalyzing factors:")
for f in factor_cols:
    print(f" • {f}")

# === Helper function: run regression and return summary ===
def run_regression(y, x):
    X = sm.add_constant(x)  # Adds intercept
    model = sm.OLS(y, X).fit()
    return {
        "Beta": model.params[1],
        "Alpha": model.params[0],
        "R²": model.rsquared,
        "t-stat": model.tvalues[1],
        "p-value": model.pvalues[1],
        "Corr": np.corrcoef(y, x)[0, 1]
    }

# === Run regressions ===
results = []

for factor in factor_cols:
    for spread in itraxx_cols:
        reg = run_regression(returns[factor], returns[spread])
        reg["Factor"] = factor
        reg["Credit_Spread"] = spread
        results.append(reg)

results_df = pd.DataFrame(results)[["Factor", "Credit_Spread", "Beta", "R²", "Corr", "t-stat", "p-value", "Alpha"]]

# === Display summary ===
print("\n=== Regression Results (Factor vs Credit) ===\n")
print(results_df.round(3))

# === Save output ===
output_file = os.path.join(folder, "factor_credit_results.csv")
results_df.to_csv(output_file, index=False)
print(f"\n✅ Saved detailed results to: {output_file}")

# === Visualization ===
for factor in factor_cols:
    for spread in itraxx_cols:
        plt.figure(figsize=(6,4))
        plt.scatter(returns[spread], returns[factor], alpha=0.5)
        plt.xlabel(f"{spread} Daily Return")
        plt.ylabel(f"{factor} Daily Return")
        plt.title(f"{factor} vs {spread}\nBeta: {results_df[(results_df['Factor']==factor)&(results_df['Credit_Spread']==spread)]['Beta'].values[0]:.2f}")
        plt.grid(True)
        plt.show()

"""
===========================================================
            PFIC ETF Cross-Check & Dividend Impact
===========================================================

Welcome! This notebook helps you find whether likely PFIC
companies appear in ETFs you're tracking, and whether those
ETFs have seen dividend changes — which could hint at
future deletions or basis trade opportunities.

-----------------------------------------------------------
🧭  HOW TO USE THIS:
-----------------------------------------------------------

1️⃣ FILES YOU NEED (from Bloomberg):

(a) bloomberg_pfic.csv
   This is your PFIC screening input.
   You already have this from earlier.
   It should include:
       Ticker, Company Name, Sector,
       Market Cap, Total Revenue,
       Investment Income, Total Assets, Investment Assets

(b) etf_holdings.csv
   This contains the ETF constituents.
   You can export this from Bloomberg by typing:
       <ETF Ticker>  HDS <GO>
   and then “Export to Excel”.
   Format the CSV like this:
       ETF Ticker, ETF Name, Constituent Ticker, Constituent Name, Weight (%)

(c) (Optional) etf_dividends.csv
   You can export this using:
       <ETF Ticker>  DVD <GO>
   Include:
       ETF Ticker, ETF Name, Dividend Yield (12M), Dividend Change (YoY)

2️⃣ Place all files in the SAME folder as your notebook.
   Example:
       PFIC Screen/
       ├── PFIC_Screener.ipynb
       ├── bloomberg_pfic.csv
       ├── etf_holdings.csv
       └── etf_dividends.csv   (optional)

3️⃣ Run this cell in Jupyter.

-----------------------------------------------------------
💡  WHAT IT DOES:
-----------------------------------------------------------

• Loads your PFIC company list
• Calculates which companies look like PFICs
• Loads ETF holdings and checks if any PFICs are held
• Calculates total PFIC weight per ETF
• (Optionally) Adds dividend data to compare
• Displays results in tables and plots

-----------------------------------------------------------
⚙️  OUTPUT:
-----------------------------------------------------------

✅ Summary table of PFIC exposure per ETF
✅ Optional correlation of PFIC exposure vs dividend change
✅ Plots for easy visual analysis
✅ Trade insight: which ETFs might face deletions or basis widening
===========================================================
"""

# === IMPORT REQUIRED PACKAGES ===
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === STEP 1: LOAD YOUR FILES ===
pfic_file = "bloomberg_pfic.csv"   # PFIC screening data
etf_file = "etf_holdings.csv"      # ETF holdings data
div_file = "etf_dividends.csv"     # (Optional) ETF dividend data

# Check all files exist before proceeding
for f in [pfic_file, etf_file]:
    if not os.path.exists(f):
        raise FileNotFoundError(f"❌ Could not find '{f}'. Please make sure it’s in the same folder as this notebook.")

print("✅ All required files found!\n")

# === STEP 2: LOAD AND CLEAN PFIC DATA ===
df = pd.read_csv(pfic_file)
df.columns = df.columns.str.strip()

# Make sure the key numeric columns are numbers
for col in ["Total Revenue", "Investment Income", "Total Assets", "Investment Assets"]:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False)
        .replace("nan", np.nan)
        .astype(float)
    )

# Calculate ratios
df["Passive Income %"] = df["Investment Income"] / df["Total Revenue"]
df["Passive Asset %"] = df["Investment Assets"] / df["Total Assets"]

# Define thresholds (PFIC test)
INCOME_THRESHOLD = 0.75
ASSET_THRESHOLD = 0.50

# Mark companies likely PFIC
df["Likely PFIC"] = (
    (df["Passive Income %"] >= INCOME_THRESHOLD) |
    (df["Passive Asset %"] >= ASSET_THRESHOLD)
)

# Extract only PFIC tickers
pfic_list = df[df["Likely PFIC"]]["Ticker"].unique()
print(f"📊 {len(pfic_list)} likely PFIC companies identified.\n")

# === STEP 3: LOAD ETF HOLDINGS DATA ===
etf = pd.read_csv(etf_file)
etf.columns = etf.columns.str.strip()

# Clean up ticker formats
etf["Constituent Ticker"] = etf["Constituent Ticker"].astype(str).str.upper().str.strip()
df["Ticker"] = df["Ticker"].astype(str).str.upper().str.strip()

# Merge ETF holdings with PFIC list
merged = etf.merge(
    df[["Ticker", "Company Name", "Likely PFIC"]],
    left_on="Constituent Ticker",
    right_on="Ticker",
    how="left"
)

# If a match was found, mark it
merged["PFIC Exposure"] = merged["Likely PFIC"].fillna(False)

# === STEP 4: ETF-LEVEL SUMMARY ===
etf_summary = (
    merged.groupby(["ETF Ticker", "ETF Name"])
    .agg(
        Total_Constituents=("Constituent Ticker", "count"),
        PFIC_Count=("PFIC Exposure", "sum"),
        PFIC_Weight=("Weight (%)", lambda x: np.sum(x[merged.loc[x.index, "PFIC Exposure"]])),
        Total_Weight=("Weight (%)", "sum")
    )
    .reset_index()
)

# Calculate PFIC exposure as % of total ETF
etf_summary["PFIC Weight %"] = etf_summary["PFIC_Weight"] / etf_summary["Total_Weight"] * 100

print("=== ETF PFIC Exposure Summary ===")
display(etf_summary)

# === STEP 5: OPTIONAL DIVIDEND DATA MERGE ===
if os.path.exists(div_file):
    div = pd.read_csv(div_file)
    div.columns = div.columns.str.strip()
    etf_summary = etf_summary.merge(div, on=["ETF Ticker", "ETF Name"], how="left")

    print("\n=== ETF Dividend + PFIC Exposure ===")
    display(etf_summary[["ETF Ticker", "ETF Name", "PFIC Weight %", "Dividend Yield (12M)", "Dividend Change (YoY)"]])

    # === VISUAL: PFIC Exposure vs Dividend Change ===
    plt.figure(figsize=(8,6))
    plt.scatter(etf_summary["PFIC Weight %"], etf_summary["Dividend Change (YoY)"], color='purple')
    plt.xlabel("PFIC Weight in ETF (%)")
    plt.ylabel("Dividend Change (YoY)")
    plt.title("PFIC Exposure vs Dividend Change (ETF Level)")
    plt.grid(True)
    plt.show()
else:
    print("\n⚠️ No dividend file found — skipping dividend correlation.\n")

# === STEP 6: INTERPRETATION HELP ===
print("""
===========================================================
📈  INTERPRETATION GUIDE
===========================================================

• “PFIC Weight %” → The % of ETF holdings that are likely PFICs.
   Example: If PFIC Weight % = 8%, roughly 8% of that ETF
   is exposed to names that might be deleted.

• “Dividend Change (YoY)” → The year-over-year change in the ETF’s
   dividend yield. If it rises while PFIC exposure drops, that could
   indicate deletions and a tightening basis.

💡 TRADE IDEA:
   - ETFs with high PFIC exposure → potential deletions → rising basis
   - Consider long futures vs short physical ETF as a hedge or spread.

===========================================================
✅ Analysis complete!
===========================================================
""")

"""
===========================================================
            PFIC Screener with Visual Analysis
===========================================================

INSTRUCTIONS:

1. Place this Jupyter Notebook and your Bloomberg export
   in the same folder, for example:

       PFIC Screen/
       ├── PFIC_Screener.ipynb
       └── bloomberg_pfic.csv

2. Your CSV file must include the following columns:

   | Column Name        | Example                 | Description |
   |--------------------|-------------------------|--------------|
   | Ticker             | AAPL US                 | Company identifier |
   | Company Name       | Apple Inc               | Company name |
   | Sector             | Information Technology  | Company sector |
   | Market Cap         | 2,500,000,000,000       | Market capitalization |
   | Total Revenue      | 365,000,000,000         | Total company revenue |
   | Investment Income  | 25,000,000,000          | Passive investment income |
   | Total Assets       | 400,000,000,000         | Total assets |
   | Investment Assets  | 210,000,000,000         | Passive investment assets |

3. Save it as:
       bloomberg_pfic.csv

4. Run this entire cell in Jupyter Notebook.

Output:
   ✅ Clean table of PFIC likelihood
   📊 Charts to visualize passive ratios
   💾 CSV output: pfic_results.csv
===========================================================
"""

# === IMPORTS ===
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

# === FILE PATH ===
file_path = "bloomberg_pfic.csv"

if not os.path.exists(file_path):
    raise FileNotFoundError(
        f"❌ File not found. Please ensure 'bloomberg_pfic.csv' "
        "is in the same folder as this notebook."
    )

print(f"✅ Found file: {file_path}")

# === LOAD DATA ===
df = pd.read_csv(file_path)
df.columns = df.columns.str.strip()

# === REQUIRED COLUMNS ===
required_cols = [
    "Ticker", "Company Name", "Sector",
    "Total Revenue", "Investment Income", "Total Assets", "Investment Assets"
]

missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise ValueError(f"❌ Missing columns in CSV: {missing}")

# === CLEAN NUMERIC COLUMNS ===
numeric_cols = ["Total Revenue", "Investment Income", "Total Assets", "Investment Assets"]
for col in numeric_cols:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False)
        .replace("nan", np.nan)
        .astype(float)
    )

# === CALCULATE RATIOS ===
df["Passive Income %"] = df["Investment Income"] / df["Total Revenue"]
df["Passive Asset %"] = df["Investment Assets"] / df["Total Assets"]

# === PFIC THRESHOLDS ===
INCOME_THRESHOLD = 0.75
ASSET_THRESHOLD = 0.50

# === PFIC FLAG LOGIC ===
def check_pfic(row):
    income_flag = row["Passive Income %"] >= INCOME_THRESHOLD if pd.notna(row["Passive Income %"]) else False
    asset_flag = row["Passive Asset %"] >= ASSET_THRESHOLD if pd.notna(row["Passive Asset %"]) else False
    return income_flag or asset_flag

df["Likely PFIC"] = df.apply(check_pfic, axis=1)

# === REASON EXPLANATION ===
def reason(row):
    reasons = []
    if pd.notna(row["Passive Income %"]) and row["Passive Income %"] >= INCOME_THRESHOLD:
        reasons.append("Passive income ≥ 75%")
    if pd.notna(row["Passive Asset %"]) and row["Passive Asset %"] >= ASSET_THRESHOLD:
        reasons.append("Passive assets ≥ 50%")
    if not reasons:
        reasons.append("Below PFIC thresholds")
    return "; ".join(reasons)

df["Reason"] = df.apply(reason, axis=1)

# === SAVE RESULTS ===
output_path = "pfic_results.csv"
df.to_csv(output_path, index=False)
print(f"\n💾 Results saved to: {output_path}\n")

# === SUMMARY OUTPUT ===
summary = df[["Ticker", "Company Name", "Sector", "Passive Income %", "Passive Asset %", "Likely PFIC", "Reason"]]
pd.set_option('display.float_format', '{:.2%}'.format)
print("=== PFIC Screening Results ===\n")
display(summary)

# === BASIC STATS ===
num_pfic = df["Likely PFIC"].sum()
total = len(df)
print(f"\n⚠️  {num_pfic} out of {total} companies flagged as likely PFICs.")

# === VISUALS ===
plt.figure(figsize=(10, 6))
plt.scatter(df["Passive Asset %"], df["Passive Income %"], c=df["Likely PFIC"].map({True: 'red', False: 'green'}))
plt.axhline(INCOME_THRESHOLD, color='gray', linestyle='--', label="75% Passive Income Threshold")
plt.axvline(ASSET_THRESHOLD, color='blue', linestyle='--', label="50% Passive Asset Threshold")
plt.title("PFIC Passive Income vs Passive Assets")
plt.xlabel("Passive Asset %")
plt.ylabel("Passive Income %")
plt.legend()
plt.grid(True)
plt.show()

# === BAR CHART (TOP 15 BY PASSIVE ASSETS) ===
top15 = df.sort_values("Passive Asset %", ascending=False).head(15)
plt.figure(figsize=(10, 6))
plt.barh(top15["Company Name"], top15["Passive Asset %"], color=np.where(top15["Likely PFIC"], 'red', 'green'))
plt.xlabel("Passive Asset %")
plt.title("Top 15 Companies by Passive Asset Ratio")
plt.gca().invert_yaxis()
plt.show()



"""
===========================================================
        PFIC Screener using Bloomberg CSV Export
        (for folder: PFIC Screen/)
===========================================================

INSTRUCTIONS:

1. Make sure your Jupyter Notebook and your Bloomberg CSV file are BOTH
   in the same folder, e.g.:

       PFIC Screen/
       ├── PFIC_Screener.ipynb
       └── bloomberg_pfic.csv

2. The Bloomberg export must include these columns:

   | Bloomberg Field Name      | Mnemonic                | Column Name in CSV (exact) |
   |---------------------------|--------------------------|-----------------------------|
   | Ticker                   | TICKER                   | Ticker                      |
   | Company Name              | NAME                     | Company Name                |
   | Country of Domicile       | CNTRY_DOMICILE           | Country Domicile            |
   | GICS Sector Name          | GICS_SECTOR_NAME         | Sector                      |
   | Market Cap (USD)          | CUR_MKT_CAP              | Market Cap                  |
   | Total Revenue             | IS_TOT_REV               | Total Revenue               |
   | Investment Income         | IS_INVESTMENT_INCOME     | Investment Income           |
   | Total Assets              | BS_TOT_ASSET             | Total Assets                |
   | Investment Assets         | BS_INVESTMENTS_ASSETS    | Investment Assets           |

3. Save the file as:  bloomberg_pfic.csv
   (no spaces, correct extension)

4. Run this entire code block.

5. Output:
   - Printed summary in the notebook
   - Full results saved as: pfic_results.csv

===========================================================
"""

import pandas as pd
import numpy as np
import os

# === Locate CSV File Automatically ===
folder = os.path.dirname(os.path.abspath(__file__)) if "__file__" in globals() else os.getcwd()
file_path = os.path.join(folder, "bloomberg_pfic.csv")

if not os.path.exists(file_path):
    raise FileNotFoundError(
        f"❌ Could not find 'bloomberg_pfic.csv' in this folder:\n{folder}\n\n"
        "➡️ Please confirm the file is in the same folder as your notebook."
    )

print(f"✅ Found Bloomberg file at: {file_path}")

# === SETTINGS ===
PASSIVE_INCOME_THRESHOLD = 0.75   # 75% passive income
PASSIVE_ASSET_THRESHOLD = 0.50    # 50% passive assets

# === LOAD DATA ===
df = pd.read_csv(file_path)
df.columns = df.columns.str.strip()  # clean up headers

# === CHECK REQUIRED COLUMNS ===
required = [
    "Ticker", "Company Name", "Country Domicile", "Sector",
    "Total Revenue", "Investment Income", "Total Assets", "Investment Assets"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"❌ Missing columns in your CSV file: {missing}\n"
                     f"Please ensure headers exactly match those listed above.")

# === CLEAN NUMERIC FIELDS ===
for col in ["Total Revenue", "Investment Income", "Total Assets", "Investment Assets"]:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False)
        .replace("nan", np.nan)
        .astype(float)
    )

# === CALCULATE RATIOS ===
df["Passive Income %"] = df["Investment Income"] / df["Total Revenue"]
df["Passive Asset %"] = df["Investment Assets"] / df["Total Assets"]

# === PFIC LOGIC ===
def is_foreign(country):
    """Return True if not US-domiciled."""
    if pd.isna(country):
        return False
    text = str(country).lower()
    return not any(word in text for word in ["united states", "us", "u.s.", "america"])

def likely_pfic(row):
    """Flag potential PFIC based on thresholds."""
    if not is_foreign(row["Country Domicile"]):
        return False
    income_flag = (
        row["Passive Income %"] >= PASSIVE_INCOME_THRESHOLD
        if pd.notna(row["Passive Income %"])
        else False
    )
    asset_flag = (
        row["Passive Asset %"] >= PASSIVE_ASSET_THRESHOLD
        if pd.notna(row["Passive Asset %"])
        else False
    )
    return income_flag or asset_flag

df["Likely PFIC"] = df.apply(likely_pfic, axis=1)

# === ADD EXPLANATION ===
def reason(row):
    if not is_foreign(row["Country Domicile"]):
        return "US-domiciled"
    reasons = []
    if pd.notna(row["Passive Income %"]) and row["Passive Income %"] >= PASSIVE_INCOME_THRESHOLD:
        reasons.append("Passive income ≥ 75%")
    if pd.notna(row["Passive Asset %"]) and row["Passive Asset %"] >= PASSIVE_ASSET_THRESHOLD:
        reasons.append("Passive assets ≥ 50%")
    if not reasons:
        reasons.append("Foreign but below thresholds")
    return "; ".join(reasons)

df["Reason"] = df.apply(reason, axis=1)

# === DISPLAY RESULTS ===
print("\n=== PFIC Screening Summary ===\n")
print(df[["Ticker", "Company Name", "Country Domicile", "Sector",
          "Passive Income %", "Passive Asset %",
          "Likely PFIC", "Reason"]])

# === SAVE OUTPUT ===
output_file = os.path.join(folder, "pfic_results.csv")
df.to_csv(output_file, index=False)
print(f"\n✅ Full results saved to: {output_file}")

# === PRINT FLAGGED COMPANIES ===
flagged = df[df["Likely PFIC"]]
print(f"\n⚠️  {len(flagged)} potential PFIC(s) flagged:\n")
print(flagged[["Ticker", "Company Name", "Country Domicile", "Reason"]])

print("\n✅ Done. Review 'pfic_results.csv' for full details.")



"""
===========================================================
        PFIC Screener using Bloomberg CSV Export
===========================================================

INSTRUCTIONS:

1. In Bloomberg Excel Add-In, create a file called:
      bloomberg_pfic.csv

2. Export the following fields (including tickers):

   | Bloomberg Field Name      | Mnemonic                | Column Name in CSV (exact) | Purpose |
   |---------------------------|--------------------------|-----------------------------|----------|
   | Ticker                   | TICKER                   | Ticker                      | Identifier |
   | Company Name              | NAME                     | Company Name                | Identify the company |
   | Country of Domicile       | CNTRY_DOMICILE           | Country Domicile            | PFIC test → must be non-US |
   | GICS Sector Name          | GICS_SECTOR_NAME         | Sector                      | Context check |
   | Market Cap (USD)          | CUR_MKT_CAP              | Market Cap                  | Context |
   | Total Revenue             | IS_TOT_REV               | Total Revenue               | Denominator for income ratio |
   | Investment Income         | IS_INVESTMENT_INCOME     | Investment Income           | Passive income numerator |
   | Total Assets              | BS_TOT_ASSET             | Total Assets                | Denominator for asset ratio |
   | Investment Assets         | BS_INVESTMENTS_ASSETS    | Investment Assets           | Passive asset numerator |

3. Save that file in the SAME folder as this notebook with the name:
      bloomberg_pfic.csv

4. Run this entire code block.

5. Output:
   - Printed summary in the notebook
   - Full results saved to: pfic_results.csv

INTERPRETATION:
   - "Likely PFIC" = True means:
       • The company is foreign-domiciled, AND
       • Either:
           - ≥75% of income is passive, or
           - ≥50% of assets are passive
   - "Reason" column explains why it was flagged.

===========================================================
"""

# === Imports ===
import pandas as pd
import numpy as np

# === SETTINGS ===
FILE_NAME = "bloomberg_pfic.csv"
PASSIVE_INCOME_THRESHOLD = 0.75   # 75% passive income
PASSIVE_ASSET_THRESHOLD = 0.50    # 50% passive assets

# === LOAD DATA ===
df = pd.read_csv(FILE_NAME)
df.columns = df.columns.str.strip()  # remove extra spaces

# === CHECK REQUIRED COLUMNS ===
required = [
    "Ticker", "Company Name", "Country Domicile", "Sector",
    "Total Revenue", "Investment Income", "Total Assets", "Investment Assets"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"❌ Missing columns in your CSV file: {missing}\n"
                     f"Please ensure headers exactly match those listed above.")

# === CLEAN NUMERIC FIELDS ===
for col in ["Total Revenue", "Investment Income", "Total Assets", "Investment Assets"]:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False)
        .replace("nan", np.nan)
        .astype(float)
    )

# === CALCULATE RATIOS ===
df["Passive Income %"] = df["Investment Income"] / df["Total Revenue"]
df["Passive Asset %"] = df["Investment Assets"] / df["Total Assets"]

# === PFIC LOGIC ===
def is_foreign(country):
    """Return True if not US-domiciled."""
    if pd.isna(country):
        return False
    text = str(country).lower()
    return not any(word in text for word in ["united states", "us", "u.s.", "america"])

def likely_pfic(row):
    """Flag potential PFIC based on thresholds."""
    if not is_foreign(row["Country Domicile"]):
        return False
    income_flag = (
        row["Passive Income %"] >= PASSIVE_INCOME_THRESHOLD
        if pd.notna(row["Passive Income %"])
        else False
    )
    asset_flag = (
        row["Passive Asset %"] >= PASSIVE_ASSET_THRESHOLD
        if pd.notna(row["Passive Asset %"])
        else False
    )
    return income_flag or asset_flag

df["Likely PFIC"] = df.apply(likely_pfic, axis=1)

# === ADD EXPLANATION ===
def reason(row):
    if not is_foreign(row["Country Domicile"]):
        return "US-domiciled"
    reasons = []
    if pd.notna(row["Passive Income %"]) and row["Passive Income %"] >= PASSIVE_INCOME_THRESHOLD:
        reasons.append("Passive income ≥ 75%")
    if pd.notna(row["Passive Asset %"]) and row["Passive Asset %"] >= PASSIVE_ASSET_THRESHOLD:
        reasons.append("Passive assets ≥ 50%")
    if not reasons:
        reasons.append("Foreign but below thresholds")
    return "; ".join(reasons)

df["Reason"] = df.apply(reason, axis=1)

# === DISPLAY RESULTS ===
print("\n=== PFIC Screening Summary ===\n")
print(df[["Ticker", "Company Name", "Country Domicile", "Sector",
          "Passive Income %", "Passive Asset %",
          "Likely PFIC", "Reason"]])

# === SAVE OUTPUT ===
output_file = "pfic_results.csv"
df.to_csv(output_file, index=False)
print(f"\n✅ Full results saved to: {output_file}")

# === PRINT FLAGGED COMPANIES ===
flagged = df[df["Likely PFIC"]]
print(f"\n⚠️  {len(flagged)} potential PFIC(s) flagged:\n")
print(flagged[["Ticker", "Company Name", "Country Domicile", "Reason"]])

print("\n✅ Done. Review 'pfic_results.csv' for full details.")

"""
===========================================================
     PFIC Screener using Bloomberg Excel Export (Simplified)
===========================================================

INSTRUCTIONS:

1. In Bloomberg Excel Add-In, create a file called:
      bloomberg_pfic.xlsx

2. Export the following fields for all the companies you want to test:

   | Bloomberg Field Name      | Mnemonic                | Column Name in Excel (exact) | Purpose |
   |---------------------------|--------------------------|-------------------------------|----------|
   | Company Name              | NAME                     | Company Name                  | Identify the company |
   | Country of Domicile       | CNTRY_DOMICILE           | Country Domicile              | PFIC test → must be non-US |
   | GICS Sector Name          | GICS_SECTOR_NAME         | Sector                        | Context check |
   | Market Cap (USD)          | CUR_MKT_CAP              | Market Cap                    | Context |
   | Total Revenue             | IS_TOT_REV               | Total Revenue                 | Denominator for income ratio |
   | Investment Income         | IS_INVESTMENT_INCOME     | Investment Income             | Passive income numerator |
   | Total Assets              | BS_TOT_ASSET             | Total Assets                  | Denominator for asset ratio |
   | Investment Assets         | BS_INVESTMENTS_ASSETS    | Investment Assets             | Passive asset numerator |

3. Save that Excel file in the SAME folder as this notebook with the name:
      bloomberg_pfic.xlsx

4. Run this entire code block.

5. Output:
   - Printed summary in the notebook
   - Full file saved as: pfic_results.xlsx

INTERPRETATION:
   - "Likely PFIC" = True means:
       - The company is foreign-domiciled, AND
       - Either:
            • ≥75% of income is passive, or
            • ≥50% of assets are passive
   - "Reason" column explains why it was flagged.

===========================================================
"""

# === Imports ===
import pandas as pd
import numpy as np

# === SETTINGS ===
FILE_NAME = "bloomberg_pfic.xlsx"
PASSIVE_INCOME_THRESHOLD = 0.75   # 75% passive income
PASSIVE_ASSET_THRESHOLD = 0.50    # 50% passive assets

# === LOAD DATA ===
df = pd.read_excel(FILE_NAME)
df.columns = df.columns.str.strip()  # remove stray spaces from headers

# === CHECK REQUIRED COLUMNS ===
required = [
    "Company Name", "Country Domicile", "Sector",
    "Total Revenue", "Investment Income", "Total Assets", "Investment Assets"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"❌ Missing columns in your Excel file: {missing}\n"
                     f"Please ensure your headers exactly match the required format.")

# === CLEAN NUMERIC FIELDS ===
for col in ["Total Revenue", "Investment Income", "Total Assets", "Investment Assets"]:
    df[col] = (
        df[col]
        .astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False)
        .replace("nan", np.nan)
        .astype(float)
    )

# === CALCULATE RATIOS ===
df["Passive Income %"] = df["Investment Income"] / df["Total Revenue"]
df["Passive Asset %"] = df["Investment Assets"] / df["Total Assets"]

# === PFIC LOGIC FUNCTIONS ===
def is_foreign(country):
    """Return True if not US-domiciled."""
    if pd.isna(country):
        return False
    text = str(country).lower()
    return not any(word in text for word in ["united states", "us", "u.s.", "america"])

def likely_pfic(row):
    """Flag potential PFIC based on thresholds."""
    if not is_foreign(row["Country Domicile"]):
        return False
    income_flag = (
        row["Passive Income %"] >= PASSIVE_INCOME_THRESHOLD
        if pd.notna(row["Passive Income %"])
        else False
    )
    asset_flag = (
        row["Passive Asset %"] >= PASSIVE_ASSET_THRESHOLD
        if pd.notna(row["Passive Asset %"])
        else False
    )
    return income_flag or asset_flag

df["Likely PFIC"] = df.apply(likely_pfic, axis=1)

# === ADD TEXT EXPLANATION ===
def reason(row):
    if not is_foreign(row["Country Domicile"]):
        return "US-domiciled"
    reasons = []
    if pd.notna(row["Passive Income %"]) and row["Passive Income %"] >= PASSIVE_INCOME_THRESHOLD:
        reasons.append("Passive income ≥ 75%")
    if pd.notna(row["Passive Asset %"]) and row["Passive Asset %"] >= PASSIVE_ASSET_THRESHOLD:
        reasons.append("Passive assets ≥ 50%")
    if not reasons:
        reasons.append("Foreign but below thresholds")
    return "; ".join(reasons)

df["Reason"] = df.apply(reason, axis=1)

# === DISPLAY RESULTS ===
print("\n=== PFIC Screening Summary ===\n")
print(df[["Company Name", "Country Domicile", "Sector",
          "Passive Income %", "Passive Asset %",
          "Likely PFIC", "Reason"]])

# === SAVE OUTPUT ===
output_file = "pfic_results.xlsx"
df.to_excel(output_file, index=False)
print(f"\n✅ Full results saved to: {output_file}")

# === PRINT FLAGGED COMPANIES ===
flagged = df[df["Likely PFIC"]]
print(f"\n⚠️  {len(flagged)} potential PFIC(s) flagged:\n")
print(flagged[["Company Name", "Country Domicile", "Reason"]])

print("\n✅ Done. Review 'pfic_results.xlsx' for full details.")


# ================================================================
# PFIC Screener for Bloomberg Excel Exports
# Author: [Your Name]
# ================================================================

import pandas as pd
import numpy as np

# === Step 1: Load Bloomberg Excel File ===
file_path = "bloomberg_data.xlsx"  # ensure file is in same folder
df = pd.read_excel(file_path)

# === Step 2: Clean Column Names ===
df.columns = df.columns.str.strip().str.lower()

# === Step 3: Check Required Columns ===
required = ['company name', 'market cap', 'country domicile', 'gics sector name']
for col in required:
    if col not in df.columns:
        raise ValueError(f"Missing required column: '{col}'. Please ensure your Excel matches the format.")

# === Step 4: Calculate Passive Ratios (if numeric data exists) ===
def safe_ratio(numerator, denominator):
    try:
        return float(numerator) / float(denominator) if denominator and denominator != 0 else np.nan
    except:
        return np.nan

if 'investment income' in df.columns and 'total revenue' in df.columns:
    df['passive_income_ratio'] = df.apply(lambda x: safe_ratio(x['investment income'], x['total revenue']), axis=1)
else:
    df['passive_income_ratio'] = np.nan

if 'investment assets' in df.columns and 'total assets' in df.columns:
    df['investment_asset_ratio'] = df.apply(lambda x: safe_ratio(x['investment assets'], x['total assets']), axis=1)
else:
    df['investment_asset_ratio'] = np.nan

# === Step 5: Define PFIC Logic ===
def is_pfic(row):
    """
    Flags a company as PFIC if:
    1. It's non-US domiciled
    2. Operates in a passive/financial sector
    3. Has >75% passive income or >50% investment assets (if available)
    """
    country = str(row.get('country domicile', '')).lower()
    sector = str(row.get('gics sector name', '')).lower()

    # Rule 1: Must be foreign
    is_foreign = country not in ['united states', 'us', 'u.s.', 'usa']

    # Rule 2: Passive sectors (broad categories)
    passive_sectors = [
        'financials', 'real estate', 'reit', 'investment',
        'asset management', 'holding', 'trust', 'fund', 'capital'
    ]
    in_passive_sector = any(word in sector for word in passive_sectors)

    # Rule 3: Passive ratios (if data available)
    passive_income_ratio = row.get('passive_income_ratio', np.nan)
    investment_asset_ratio = row.get('investment_asset_ratio', np.nan)

    meets_ratio_test = (
        (not np.isnan(passive_income_ratio) and passive_income_ratio >= 0.75) or
        (not np.isnan(investment_asset_ratio) and investment_asset_ratio >= 0.5)
    )

    # Combine conditions
    if is_foreign and (in_passive_sector or meets_ratio_test):
        return True
    return False

# === Step 6: Apply Logic ===
df['likely_pfic'] = df.apply(is_pfic, axis=1)

# === Step 7: Display & Export Results ===
pfic_df = df[df['likely_pfic']].copy()

print("===================================================")
print(f"✅ PFIC Screening Complete: {len(pfic_df)} potential PFICs found.")
print("===================================================\n")

display_cols = [
    'company name', 'country domicile', 'gics sector name', 'market cap',
    'passive_income_ratio', 'investment_asset_ratio', 'likely_pfic'
]

display(df[display_cols].head(20))

# Save results
output_file = "pfic_results.xlsx"
pfic_df.to_excel(output_file, index=False)

print(f"💾 Results saved to '{output_file}'




# PFIC Forensic Screener - Detailed, copy & paste into Jupyter
# Requirements: pandas, numpy, openpyxl, country_converter (optional)
# pip install pandas numpy openpyxl country_converter

import os
import pandas as pd
import numpy as np
from pathlib import Path

# ----------------------------
# User settings
# ----------------------------
INPUT_FILE = "bloomberg_data.xlsx"   # put your Bloomberg export here
OUTPUT_EXCEL = "pfic_forensic_report.xlsx"
FLAGGED_FILE = "pfic_flagged.xlsx"
CSV_OUT = "pfic_forensic_detailed.csv"

# Thresholds per PFIC rules (statutory thresholds)
PASSIVE_INCOME_THRESHOLD = 0.75   # 75% of gross income passive
PASSIVE_ASSET_THRESHOLD = 0.50    # 50% of assets produce passive income

# Tolerances (for human review): allow tiny epsilon due to rounding
EPS = 1e-9

# ----------------------------
# Helper functions
# ----------------------------
def read_input(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Input file not found: {file_path}. Please export Bloomberg file to this name.")
    # read with pandas (supports .xlsx)
    df = pd.read_excel(file_path, engine="openpyxl")
    # normalize column names (lowercase & stripped)
    df.columns = df.columns.str.strip()
    return df

def safe_num(x):
    """Convert a cell to float or NaN (handles strings with commas, parentheses)."""
    if pd.isna(x):
        return np.nan
    try:
        # remove commas and parentheses and currency symbols
        s = str(x).replace(',', '').replace('(', '-').replace(')', '').replace('$','').strip()
        return float(s)
    except:
        return np.nan

def ensure_ratio(numer, denom):
    """Return ratio numer/denom or NaN if denom missing/0."""
    n = safe_num(numer)
    d = safe_num(denom)
    if pd.isna(n) or pd.isna(d) or d == 0:
        return np.nan
    return n / d

def isin_to_country(isin):
    """Infer country from ISIN prefix (first 2 letters) if present."""
    if pd.isna(isin):
        return None
    try:
        return isin.strip()[:2].upper()
    except:
        return None

def is_us_country_text(s):
    if pd.isna(s):
        return False
    s2 = str(s).lower()
    return any(x in s2 for x in ['united states','usa','u.s.','us','america'])

# ----------------------------
# Load data
# ----------------------------
print("Loading input file:", INPUT_FILE)
df_raw = read_input(INPUT_FILE)
print(f"Loaded {len(df_raw)} rows and columns: {list(df_raw.columns)}")

# Standardize column names for internal use (lowercase)
col_map = {c: c.strip() for c in df_raw.columns}
df = df_raw.rename(columns=col_map)
# We'll try to map commonly used names to canonical internal column names
# Preferred exact names as per instructions above; but attempt flexible mapping
canonical = {
    'company name':'Company Name',
    'name':'Company Name',
    'ticker':'Ticker',
    'id_isin':'ISIN',
    'isin':'ISIN',
    'cntry_domicile':'Country Domicile',
    'country domicile':'Country Domicile',
    'incorp_country':'Incorporation Country',
    'incorporation country':'Incorporation Country',
    'cur_mkt_cap':'Market Cap',
    'market cap':'Market Cap',
    'bs_tot_asset':'Total Assets',
    'total assets':'Total Assets',
    'bs_investments_assets':'Investment Assets',
    'investment assets':'Investment Assets',
    'is_tot_rev':'Total Revenue',
    'total revenue':'Total Revenue',
    'is_investment_income':'Investment Income',
    'investment income':'Investment Income',
    'is_interest_income':'Interest Income',
    'is_non_op_inc':'Non-op Income',
    'gics_sector_name':'GICS Sector Name',
    'gics sector name':'GICS Sector Name',
    'free_float':'Free Float',
    'free float':'Free Float',
    'iss_out':'Shares Outstanding',
    'shares_outstanding':'Shares Outstanding',
    'exchange country':'Exchange Country',
    'exch_country':'Exchange Country'
}

# perform mapping if columns exist (case-insensitive)
cols_lower = {c.lower(): c for c in df.columns}
for key, canon in canonical.items():
    if key in cols_lower:
        df = df.rename(columns={cols_lower[key]: canon})

# Now check required minimal columns
required_min = ['Company Name', 'Market Cap', 'GICS Sector Name']
missing_req = [c for c in required_min if c not in df.columns]
if missing_req:
    print("Warning: your Bloomberg file is missing recommended columns:", missing_req)
    print("You can still run the script, but some checks will be 'N/A' and require manual inspection.")

# Prepare output DataFrame with canonical cols (create missing with NaN)
for c in ['Company Name','Ticker','ISIN','Country Domicile','Incorporation Country',
          'Market Cap','Total Assets','Investment Assets','Total Revenue','Investment Income',
          'Interest Income','Non-op Income','GICS Sector Name','Free Float','Shares Outstanding','Exchange Country']:
    if c not in df.columns:
        df[c] = np.nan

# Convert numeric columns
for numcol in ['Market Cap','Total Assets','Investment Assets','Total Revenue','Investment Income','Interest Income','Free Float','Shares Outstanding']:
    df[numcol] = df[numcol].apply(safe_num)

# Normalize Free Float if in percent 0-100 -> convert to 0-1
if 'Free Float' in df.columns:
    df['Free Float'] = df['Free Float'].apply(lambda x: (x/100.0) if (not pd.isna(x) and x>1.5) else x)

# ----------------------------
# Compute forensic PFIC metrics & checklist
# ----------------------------
def pfic_check_row(row):
    reasons = []
    flags = {}
    # 1) Domicile: check Country Domicile, Incorporation Country, ISIN prefix
    country_text = row.get('Country Domicile')
    inc_country = row.get('Incorporation Country')
    isin = row.get('ISIN')
    inferred_isin_country = isin_to_country(isin)
    # US test
    is_us_dom = is_us_country_text(country_text) or is_us_country_text(inc_country)
    if inferred_isin_country:
        if inferred_isin_country in ['US','US']: # explicit
            is_us_dom = True
    flags['is_foreign_domicile'] = not is_us_dom
    if not is_us_dom:
        reasons.append("Foreign domicile (Country Domicile / Incorporation / ISIN indicates non-US)")
    else:
        reasons.append("US domicile flagged (not foreign)")
    # 2) Passive income ratio
    passive_income_ratio = ensure_ratio(row.get('Investment Income'), row.get('Total Revenue'))
    # also include interest + non-op income if Investment Income missing
    if pd.isna(passive_income_ratio):
        alt_num = 0.0
        got_alt = False
        if not pd.isna(row.get('Interest Income')):
            alt_num += safe_num(row.get('Interest Income')); got_alt=True
        if not pd.isna(row.get('Non-op Income')):
            alt_num += safe_num(row.get('Non-op Income')); got_alt=True
        if got_alt and not pd.isna(row.get('Total Revenue')):
            passive_income_ratio = safe_num(alt_num) / safe_num(row.get('Total Revenue'))
    flags['passive_income_ratio'] = passive_income_ratio
    if not pd.isna(passive_income_ratio):
        if passive_income_ratio + EPS >= PASSIVE_INCOME_THRESHOLD:
            flags['passive_income_flag'] = True
            reasons.append(f"Passive income ratio >= {PASSIVE_INCOME_THRESHOLD:.0%} ({passive_income_ratio:.2%})")
        else:
            flags['passive_income_flag'] = False
            reasons.append(f"Passive income ratio below threshold ({passive_income_ratio:.2%})")
    else:
        flags['passive_income_flag'] = None
        reasons.append("Passive income ratio N/A (check Investment Income / Total Revenue fields)")
    # 3) Investment asset ratio
    inv_asset_ratio = ensure_ratio(row.get('Investment Assets'), row.get('Total Assets'))
    flags['investment_asset_ratio'] = inv_asset_ratio
    if not pd.isna(inv_asset_ratio):
        if inv_asset_ratio + EPS >= PASSIVE_ASSET_THRESHOLD:
            flags['passive_assets_flag'] = True
            reasons.append(f"Investment asset ratio >= {PASSIVE_ASSET_THRESHOLD:.0%} ({inv_asset_ratio:.2%})")
        else:
            flags['passive_assets_flag'] = False
            reasons.append(f"Investment asset ratio below threshold ({inv_asset_ratio:.2%})")
    else:
        flags['passive_assets_flag'] = None
        reasons.append("Investment asset ratio N/A (check Investment Assets / Total Assets fields)")
    # 4) Passive sector heuristic (REIT, Fund, Holding, Investment, Trust)
    sector = str(row.get('GICS Sector Name', '')).lower() if not pd.isna(row.get('GICS Sector Name')) else ''
    passive_sector_keywords = ['financials','real estate','reit','investment','asset management','holding','trust','fund','capital','private equity','venture']
    in_passive_sector = any(k in sector for k in passive_sector_keywords)
    flags['passive_sector'] = in_passive_sector
    reasons.append("Passive-sector signal" if in_passive_sector else "Non-passive sector signal")
    # 5) Market cap small / suspicious (optional heuristic)
    mktcap = safe_num(row.get('Market Cap'))
    flags['market_cap'] = mktcap
    if not pd.isna(mktcap) and mktcap < 50e6:
        reasons.append("Very small market cap (< $50m) - inspect for private/illiquid company")
    # 6) Free float small (optional)
    ff = row.get('Free Float')
    flags['free_float'] = ff
    if not pd.isna(ff):
        if ff < 0.05:
            reasons.append("Very low free float (<5%) - potential lock-up or strategic holdings")
        elif ff < 0.20:
            reasons.append("Low free float (<20%) - reduced public float")
    # 7) Final PFIC estimation logic for initial screening
    likely_pfic = False
    # statutory style test: foreign AND (passive income threshold OR passive asset threshold)
    if flags['is_foreign_domicile'] and ((flags['passive_income_flag'] is True) or (flags['passive_assets_flag'] is True)):
        likely_pfic = True
        reasons.append("Meets statutory-like PFIC thresholds (foreign + passive ratios)")
    # heuristic flag: foreign + passive sector or very low free float (warrants manual review)
    elif flags['is_foreign_domicile'] and (in_passive_sector or (not pd.isna(ff) and ff < 0.20)):
        reasons.append("Heuristic PFIC candidate: foreign + passive-sector or low free float -> manual review recommended")
        likely_pfic = True
    else:
        reasons.append("Not likely PFIC by automated tests (manual check recommended if uncertain)")
    flags['likely_pfic'] = likely_pfic
    flags['reasons'] = " ; ".join(reasons)
    # include computed ratios for output
    flags['passive_income_ratio_display'] = (f"{passive_income_ratio:.4f}" if not pd.isna(passive_income_ratio) else "N/A")
    flags['investment_asset_ratio_display'] = (f"{inv_asset_ratio:.4f}" if not pd.isna(inv_asset_ratio) else "N/A")
    return flags

# Run row-by-row
records = []
for idx, r in df.iterrows():
    out = pfic_check_row(r)
    # produce a tidy record
    rec = {
        'Company Name': r.get('Company Name'),
        'Ticker': r.get('Ticker'),
        'ISIN': r.get('ISIN'),
        'Country Domicile': r.get('Country Domicile'),
        'Incorporation Country': r.get('Incorporation Country'),
        'Market Cap': r.get('Market Cap'),
        'GICS Sector Name': r.get('GICS Sector Name'),
        'Free Float': r.get('Free Float'),
        'Total Assets': r.get('Total Assets'),
        'Investment Assets': r.get('Investment Assets'),
        'Total Revenue': r.get('Total Revenue'),
        'Investment Income': r.get('Investment Income'),
        'Interest Income': r.get('Interest Income'),
        'Non-op Income': r.get('Non-op Income'),
        'Likely PFIC': out['likely_pfic'],
        'Passive Income Ratio': out['passive_income_ratio_display'],
        'Investment Asset Ratio': out['investment_asset_ratio_display'],
        'Passive Sector Signal': out['passive_sector'],
        'Is Foreign Domicile': out['is_foreign_domicile'],
        'Free Float (raw)': out['free_float'],
        'Checklist Reasons': out['reasons']
    }
    records.append(rec)

out_df = pd.DataFrame.from_records(records)

# Save outputs: excel with multiple sheets and a flagged-only file
with pd.ExcelWriter(OUTPUT_EXCEL, engine='openpyxl') as writer:
    out_df.to_excel(writer, sheet_name='PFIC_Forensic', index=False)
    out_df[out_df['Likely PFIC'] == True].to_excel(writer, sheet_name='Flagged_PFIC', index=False)
    out_df[out_df['Likely PFIC'] == False].to_excel(writer, sheet_name='NonFlagged', index=False)

out_df.to_csv(CSV_OUT, index=False)
out_df[out_df['Likely PFIC']].to_excel(FLAGGED_FILE, index=False)

print(f"Done. Detailed PFIC forensic report saved to: {OUTPUT_EXCEL}")
print(f"Flagged-only file saved to: {FLAGGED_FILE}")
print(f"CSV saved to: {CSV_OUT}")
print("\nSummary counts:")
print(out_df['Likely PFIC'].value_counts(dropna=False))

# Show first 10 flagged for quick inspection in notebook
flagged = out_df[out_df['Likely PFIC'] == True]
print(f"\nFirst 10 flagged PFIC candidates (showing checklist): {len(flagged)} found")
display_cols = ['Company Name','Ticker','ISIN','Country Domicile','GICS Sector Name','Market Cap','Passive Income Ratio','Investment Asset Ratio','Free Float (raw)','Checklist Reasons']
print(flagged[display_cols].head(10).to_string(index=False))

. Open in Excel to view all flagged PFICs.")