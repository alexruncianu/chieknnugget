
Key Factors Behind the Outperformance
Index Fund Effects and Liquidity Premium
The primary reason for Class B shares’ outperformance appears to be related to index fund inclusion and liquidity dynamics. As noted in Swedish investment discussions, both Class A and Class B shares are included in major indices. However, since Class B shares represent only about 2% of all outstanding shares (with Class A representing 98%), they have much lower liquidity and trading volume.
When index funds rebalance or purchase shares, they buy both classes proportionally. However, the limited supply of Class B shares creates a liquidity premium - the lower float means that relatively small purchase volumes can drive prices higher more easily than with the highly liquid Class A shares

Share Buyback Program Impact
Your mention of the February 2025 share buyback announcement is relevant. The bank’s board proposed authorization to repurchase up to 120 million Class A and/or Class B shares. This buyback authorization, combined with the extraordinary dividend of SEK 7.50 per share announced in February 2025 (in addition to the ordinary dividend of SEK 7.50), likely contributed to positive sentiment for both share classes.

However, the buyback program may have had a disproportionate impact on Class B shares due to their lower liquidity. When buyback programs target both share classes, the effect on the less liquid Class B shares tends to be more pronounced.

Market Structure Inefficiency
This performance differential highlights a classic market inefficiency. Both share classes have identical dividend rights and represent the same underlying business, with the only difference being voting rights (Class A has one vote per share, Class B has one-tenth vote per share). In an efficient market, this voting premium should be minimal, especially given that Class A shares already represent 98% of the total.



Conclusion
The outperformance of Class B shares appears to be primarily driven by market microstructure effects rather than fundamental factors. The combination of index fund purchasing patterns, lower liquidity creating a scarcity premium, and the share buyback program has created conditions where Class B shares have significantly outperformed their Class A counterparts. This represents a market inefficiency that experienced investors like yourself often identify - identical economic exposure trading at different implicit valuations due to structural factors rather than fundamental differences.

# Backtest: After-hours gapers -> trade momentum into main session
# Run in a bQuant Jupyter notebook. Adapt data fetch to your BQL / bQuant utilities.
import pandas as pd
import numpy as np
import datetime as dt
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt

# -----------------------------
# USER CONFIG
# -----------------------------
START = "2023-01-01"   # backtest start (YYYY-MM-DD)
END   = "2025-08-31"   # backtest end
UNIVERSE = "US_EQ"     # placeholder list or universe name you will provide
GAP_THRESHOLD = 0.02   # only trade gaps greater than 2% (absolute)
MIN_VOLUME = 100_000   # minimum ADV or today's after-hours volume (optional)
MAX_POSITIONS = 20     # portfolio max number of names in a day
TRADE_FEE = 0.0005     # commission per share (or percent). adjust to your firm's model
SLIPPAGE = 0.0005      # per trade slippage as fraction of price
HOLD = 1               # holding period in trading days (1 = enter at open, exit at close same day)
CAPITAL = 1_000_000    # starting capital

# -----------------------------
# Helper: Data fetch (ADAPT THIS)
# -----------------------------
def fetch_price_data(tickers: List[str], start: str, end: str) -> Dict[str, pd.DataFrame]:
    """
    Returns a dict: ticker -> DataFrame indexed by DATE with columns:
      - prev_close: previous regular session close price (close_t-1)
      - ah_last: last after-hours price (post-market; e.g., last trade after 16:00 ET)
      - open: regular session open price (next trading day open)
      - close: regular session close price (same day close)
      - volume: regular session volume (for filters)
      - ah_volume: after-hours volume (optional)
    IMPORTANT: Replace this stub with a BQL / bQuant intraday / field query that returns these fields.
    """
    # --- EXAMPLE STUB USING CSV/LOCAL DATA OR SAMPLE GENERATED DATA (for testing) ---
    # If you have real bQuant access, replace the below with a BQL query.
    # Example BQL pseudocode you'd call (conceptual):
    #   request = bql.Request(
    #       bql.data(['PX_LAST'], universe).with_session('AFTER_HOURS') ...
    #   )
    # then transform results into the DataFrame described below.
    #
    # For now: build empty structure for each ticker.
    out = {}
    dates = pd.bdate_range(start, end)
    for t in tickers:
        # random walk stub (REMOVE when using real data)
        rng = np.cumprod(1 + np.random.normal(0, 0.01, size=len(dates)))
        prev_close = 100.0 * rng
        # simulate small after-hours move
        ah_last = prev_close * (1 + np.random.normal(0, 0.02, size=len(dates)))
        open_p = prev_close * (1 + np.random.normal(0, 0.005, size=len(dates)))
        close_p = open_p * (1 + np.random.normal(0, 0.01, size=len(dates)))
        df = pd.DataFrame({
            'date': dates,
            'prev_close': prev_close,
            'ah_last': ah_last,
            'open': open_p,
            'close': close_p,
            'volume': np.random.randint(50_000, 2_000_000, size=len(dates)),
            'ah_volume': np.random.randint(0, 50_000, size=len(dates))
        }).set_index('date')
        out[t] = df
    return out

# -----------------------------
# Utility: assemble panel
# -----------------------------
def build_panel(tickers: List[str], start: str, end: str) -> pd.DataFrame:
    data = fetch_price_data(tickers, start, end)
    # Align into MultiIndex DataFrame: index=date, columns=(ticker, field)
    frames = []
    for t, df in data.items():
        # add ticker level
        df2 = df.copy()
        df2.columns = pd.MultiIndex.from_product([[t], df2.columns])
        frames.append(df2)
    panel = pd.concat(frames, axis=1).sort_index()
    return panel

# -----------------------------
# Strategy: compute gap signals and daily trades
# -----------------------------
def compute_daily_signals(panel: pd.DataFrame, gap_threshold: float) -> pd.DataFrame:
    """
    Returns DataFrame indexed by date with columns:
      - longs: list of tickers to long
      - shorts: list of tickers to short
      - signals: dict ticker -> signal (1 long, -1 short, 0 nothing)
    """
    dates = panel.index.unique()
    tickers = sorted({col[0] for col in panel.columns})
    signals_by_date = []
    for date in dates:
        # For each ticker compute gap = ah_last / prev_close - 1 (on date t)
        row = panel.loc[date]
        signal_map = {}
        longers, shorters = [], []
        for t in tickers:
            try:
                prev_close = row[(t, 'prev_close')]
                ah_last = row[(t, 'ah_last')]
                vol = row[(t, 'volume')]
            except KeyError:
                continue
            if np.isnan(prev_close) or prev_close <= 0:
                continue
            gap = (ah_last / prev_close) - 1.0
            if abs(gap) < gap_threshold:
                sig = 0
            elif gap >= gap_threshold:
                sig = 1
                longers.append(t)
            elif gap <= -gap_threshold:
                sig = -1
                shorters.append(t)
            else:
                sig = 0
            signal_map[t] = {'signal': sig, 'gap': gap, 'volume': vol}
        signals_by_date.append({
            'date': date,
            'longs': longers,
            'shorts': shorters,
            'signals': signal_map
        })
    df = pd.DataFrame(signals_by_date).set_index('date')
    return df

# -----------------------------
# Execution/backtest engine
# -----------------------------
def backtest(panel: pd.DataFrame,
             signals_df: pd.DataFrame,
             capital: float = CAPITAL,
             max_positions: int = MAX_POSITIONS,
             fee: float = TRADE_FEE,
             slippage: float = SLIPPAGE) -> pd.DataFrame:
    """
    Simple daily backtester:
      - At each date, choose up to max_positions from longs and shorts (equal weight inside each side)
      - Buy at 'open' price, sell at 'close' same day (HOLD=1)
      - Compute P&L, portfolio NAV, returns
    """
    nav = capital
    nav_hist = []
    trades = []
    dates = signals_df.index
    tickers = sorted({col[0] for col in panel.columns})

    for date in dates:
        row = panel.loc[date]
        srow = signals_df.loc[date]
        longs = srow['longs'][:]
        shorts = srow['shorts'][:]

        # limit positions
        # prioritize by magnitude of gap (largest gaps first)
        def sort_and_limit(lst, sign):
            scored = []
            for t in lst:
                gap = srow['signals'][t]['gap']
                scored.append((t, abs(gap)))
            scored = sorted(scored, key=lambda x: -x[1])
            chosen = [x[0] for x in scored[:max_positions//2]]
            return chosen

        longs = sort_and_limit(longs, 1)
        shorts = sort_and_limit(shorts, -1)

        n_long = len(longs)
        n_short = len(shorts)
        total_positions = n_long + n_short
        if total_positions == 0:
            nav_hist.append({'date': date, 'nav': nav, 'daily_return': 0.0})
            continue

        # allocate equal weight per position up to capital
        weight = nav / total_positions

        daily_pnl = 0.0
        daily_turnover = 0.0

        # execute longs
        for t in longs:
            try:
                p_open = row[(t, 'open')]
                p_close = row[(t, 'close')]
            except KeyError:
                continue
            shares = weight / (p_open * (1 + slippage))
            # entry cost
            entry_cost = shares * p_open * (1 + slippage) + shares * p_open * fee
            exit_proceeds = shares * p_close * (1 - slippage) - shares * p_close * fee
            pnl = exit_proceeds - entry_cost
            daily_pnl += pnl
            daily_turnover += shares * p_open

            trades.append({'date': date, 'ticker': t, 'side': 'long', 'open': p_open,
                           'close': p_close, 'shares': shares, 'pnl': pnl})

        # execute shorts (simplified: proceeds at open, buy-to-cover at close)
        for t in shorts:
            try:
                p_open = row[(t, 'open')]
                p_close = row[(t, 'close')]
            except KeyError:
                continue
            shares = weight / (p_open * (1 + slippage))
            # for a short: sell at open, buy back at close
            proceeds = shares * p_open * (1 - slippage) - shares * p_open * fee
            cover_cost = shares * p_close * (1 + slippage) + shares * p_close * fee
            pnl = proceeds - cover_cost
            daily_pnl += pnl
            daily_turnover += shares * p_open
            trades.append({'date': date, 'ticker': t, 'side': 'short', 'open': p_open,
                           'close': p_close, 'shares': shares, 'pnl': pnl})

        daily_return = daily_pnl / nav
        nav = nav + daily_pnl
        nav_hist.append({'date': date, 'nav': nav, 'daily_return': daily_return, 'pnl': daily_pnl, 'turnover': daily_turnover})

    nav_df = pd.DataFrame(nav_hist).set_index('date')
    trades_df = pd.DataFrame(trades)
    return nav_df, trades_df

# -----------------------------
# Performance metrics & plots
# -----------------------------
def performance_report(nav_df: pd.DataFrame, trades_df: pd.DataFrame):
    nav_df['cum_return'] = nav_df['nav'] / nav_df['nav'].iloc[0] - 1.0
    returns = nav_df['daily_return'].fillna(0.0)
    ann_return = (1 + nav_df['cum_return'].iloc[-1]) ** (252 / len(nav_df)) - 1 if len(nav_df) > 0 else np.nan
    ann_vol = returns.std() * np.sqrt(252)
    sharpe = (returns.mean() * 252) / (returns.std() * np.sqrt(252)) if returns.std() > 0 else np.nan
    total_trades = len(trades_df)
    avg_daily_turnover = nav_df['turnover'].mean() if 'turnover' in nav_df.columns else np.nan

    print("=== Performance Summary ===")
    print(f"Start NAV: {nav_df['nav'].iloc[0]:.2f}")
    print(f"End NAV:   {nav_df['nav'].iloc[-1]:.2f}")
    print(f"Total return: {nav_df['cum_return'].iloc[-1]*100:.2f}%")
    print(f"Ann. return (est): {ann_return*100:.2f}%")
    print(f"Ann. vol (est): {ann_vol*100:.2f}%")
    print(f"Sharpe (est): {sharpe:.2f}")
    print(f"Total trades: {total_trades}")
    print(f"Avg daily turnover: {avg_daily_turnover:.2f}")

    # Plot NAV
    plt.figure(figsize=(10,5))
    plt.plot(nav_df.index, nav_df['nav'])
    plt.title('NAV over time')
    plt.xlabel('Date')
    plt.ylabel('NAV')
    plt.grid(True)
    plt.show()

    # Simple P&L histogram
    if not trades_df.empty:
        plt.figure(figsize=(8,4))
        plt.hist(trades_df['pnl'], bins=50)
        plt.title('Trade PnL distribution')
        plt.xlabel('PnL')
        plt.show()

# -----------------------------
# RUN: assemble universe, build panel, compute signals, backtest
# -----------------------------
# Replace with a real list / BQL universe query in bQuant:
# e.g., tickers = bq.get_universe('US_EQUITY_LARGE_CAP') or load your list
# For demonstration, we'll create a small synthetic ticker list; replace before live run.
tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'TSLA', 'NVDA', 'META', 'NFLX', 'INTC', 'AMD']

panel = build_panel(tickers, START, END)            # ADAPT: this should use real bQuant/BQL fetch
signals_df = compute_daily_signals(panel, GAP_THRESHOLD)
nav_df, trades_df = backtest(panel, signals_df, capital=CAPITAL)

# Show results
performance_report(nav_df, trades_df)

# Optionally: display top winners / losers
if not trades_df.empty:
    print("\nTop 10 winning trades:")
    print(trades_df.sort_values('pnl', ascending=False).head(10))
    print("\nTop 10 losing trades:")
    print(trades_df.sort_values('pnl').head(10))



# Simple daily after-hours gap trading backtest for bQuant (BQL)
# Paste into a bQuant notebook. Requires bql package available in bQuant: `import bql`
# Two modes: intraday preferred (minute bars) or fallback to end-of-day fields.
# Strategy: each date form a new portfolio of top |gap| movers after-hours;
#           long gap-up names, short gap-down names, equal weight, enter at open, exit at close same day.

import bql
from bql.util import get_time_series   # available in many bQuant environments
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

# -------------------------
# User config (edit)
# -------------------------
bq = bql.Service()                # typical bQuant BQL client
START = "2023-01-01"
END   = "2025-08-31"
UNIVERSE = "SPX Index"           # Example: S&P 500 index members (change to your universe or list)
TOP_N = 20                       # target number of names each day (sum of longs+shorts)
GAP_THRESHOLD = 0.0              # absolute gap threshold (0 = take top movers by abs gap)
MIN_AVG_VOLUME = 50_000         # optional liquidity filter (daily volume)
TRANSACTION_COST_PCT = 0.0005    # percent round-trip (entry+exit modelled simply)
SLIPPAGE_PCT = 0.0005            # slippage per trade
START_CAPITAL = 1_000_000

# -------------------------
# Helper functions
# -------------------------
def get_universe_members(universe_name):
    """
    Returns a list of tickers for the universe.
    Example: 'SPX Index' -> returns S&P 500 members.
    If you prefer a static list, replace this function or return a Python list.
    """
    try:
        members = bq.univ.members(universe_name)
        # convert to Bloomberg style tickers if necessary (bql returns security objects)
        secs = [s for s in members]  # may already be strings like 'AAPL US Equity'
        return secs
    except Exception as e:
        print("Universe query failed; falling back to sample list. Error:", e)
        return ['AAPL US Equity','MSFT US Equity','AMZN US Equity','TSLA US Equity','NVDA US Equity']

def fetch_day_data(securities, date_str):
    """
    For a given list of securities and a particular date (YYYY-MM-DD),
    attempt to fetch:
      - prev_close: previous regular session close (PX_LAST at t-1)
      - after_hours_last: last trade price after 16:00 ET on date t (from intraday bars if available)
      - open: next session open (we use open on date+1 for entry)
      - close: next session close (close on date+1 for exit)
      - volume: regular session volume on date+1 (for liquidity filters)
    Returns a DataFrame indexed by security with these columns.
    NOTE: you may need to adapt intraday time window to local exchange hours or your data availability.
    """
    # parse date and compute neighbors
    date = pd.to_datetime(date_str).date()
    prev_date = (pd.to_datetime(date_str) - pd.Timedelta(days=1)).strftime('%Y-%m-%d')
    next_date = (pd.to_datetime(date_str) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')

    results = []

    # Try intraday minute bars first (preferred)
    use_intraday = True
    try:
        # get_time_series supports intraday if your bQuant has access -- 1m bars for the given day
        # we'll query minute bars for the evening window (16:00:01 to 23:59:59) on the date and take last price
        # and also request next_date open/close via daily(px_open, px_close)
        intraday = get_time_series(securities,
                                   ['PX_LAST'],
                                   start=date_str,
                                   end=date_str,
                                   barSize='1m',
                                   raw=True)
        # intraday is a dict or dataframe; handle flexibly
        # We'll attempt to extract last trade time after 16:00 and drop if not found.
    except Exception:
        use_intraday = False

    # Fallback: use daily fields only (less accurate). We'll get prev close on date and open/close on next_date.
    try:
        # BQL single request for daily fields (prev close on date, open/close on next date)
        # Using BQL string interface is often robust; get px_last for date and px_open/px_close for next_date
        # Note: if your environment supports date ranges you can shift; else run two queries.
        q_prev = f"get(PX_LAST) for({securities})"
        res_prev = bq.execute(q_prev)  # might return point-in-time; we'll extract series if available
        # Simpler approach below: use get_time_series for daily px_open/px_close for a two-day window and index appropriately
        ts = get_time_series(securities, ['PX_OPEN','PX_CLOSE','PX_LAST','VOLUME'], start=prev_date, end=next_date)
        # ts is a MultiIndex DataFrame: (security, field)
        # We'll synthesize the required columns per security
        for sec in securities:
            try:
                sec_idx = sec
                # prev_close from prev_date (PX_LAST on prev_date)
                prev_close = ts.loc[prev_date][(sec_idx,'PX_LAST')] if prev_date in ts.index else np.nan
                # after_hours_last: if intraday found, take it; otherwise use PX_LAST on date as approximation
                if use_intraday and sec in intraday:
                    # intraday[sec] is a DataFrame of minute bars; pick last price after 16:00
                    df_intr = intraday[sec]
                    if not df_intr.empty:
                        # times are likely timezone-aware; we filter by time >=16:00
                        df_intr.index = pd.to_datetime(df_intr.index)
                        df_af = df_intr[df_intr.index.time >= pd.to_datetime("16:00").time()]
                        after_hours_last = df_af['PX_LAST'].iloc[-1] if not df_af.empty else np.nan
                    else:
                        after_hours_last = np.nan
                else:
                    after_hours_last = ts.loc[date_str][(sec_idx,'PX_LAST')] if date_str in ts.index else np.nan

                # Next session open/close (entry/exit) from next_date
                open_p = ts.loc[next_date][(sec_idx,'PX_OPEN')] if next_date in ts.index else np.nan
                close_p = ts.loc[next_date][(sec_idx,'PX_CLOSE')] if next_date in ts.index else np.nan
                vol = ts.loc[next_date][(sec_idx,'VOLUME')] if next_date in ts.index else np.nan

                results.append({
                    'security': sec,
                    'prev_close': prev_close,
                    'after_hours_last': after_hours_last,
                    'open': open_p,
                    'close': close_p,
                    'volume': vol
                })
            except Exception:
                results.append({'security': sec, 'prev_close': np.nan, 'after_hours_last': np.nan, 'open': np.nan, 'close': np.nan, 'volume': np.nan})
    except Exception as e:
        print("Daily data fetch failed; error:", e)
        # Build empty frame to avoid crashing
        for sec in securities:
            results.append({'security': sec, 'prev_close': np.nan, 'after_hours_last': np.nan, 'open': np.nan, 'close': np.nan, 'volume': np.nan})

    df = pd.DataFrame(results).set_index('security')
    return df

def compute_gaps(df):
    """
    df has prev_close and after_hours_last columns; compute gap = (after_hours_last / prev_close - 1)
    """
    df = df.copy()
    df['gap'] = df['after_hours_last'] / df['prev_close'] - 1
    return df

# -------------------------
# Backtest engine (simple)
# -------------------------
def run_backtest(start, end, universe_name, top_n=TOP_N):
    # build trading calendar (business days)
    dates = pd.bdate_range(start, end).strftime('%Y-%m-%d').tolist()
    secs_master = get_universe_members(universe_name)
    nav = START_CAPITAL
    nav_hist = []
    trades = []

    for date in dates[:-1]:  # last date can't trade because we need next open/close
        try:
            # For speed: restrict to universe members (could be thousands) - you can also prefilter by ADV outside loop
            df_day = fetch_day_data(secs_master, date)
            # drop rows missing required fields
            df_day = df_day.dropna(subset=['prev_close','after_hours_last','open','close'])
            if df_day.empty:
                nav_hist.append({'date': date, 'nav': nav, 'daily_return': 0.0})
                continue

            df_day = compute_gaps(df_day)

            # Optional liquidity filter
            df_day = df_day[df_day['volume'] >= MIN_AVG_VOLUME]

            # Rank by absolute gap magnitude (largest movers)
            df_day['abs_gap'] = df_day['gap'].abs()
            df_day = df_day.sort_values('abs_gap', ascending=False)

            # Select top_n movers (if threshold > 0, only those exceeding threshold)
            if GAP_THRESHOLD > 0:
                df_sel = df_day[df_day['abs_gap'] >= GAP_THRESHOLD].head(top_n)
            else:
                df_sel = df_day.head(top_n)

            if df_sel.empty:
                nav_hist.append({'date': date, 'nav': nav, 'daily_return': 0.0})
                continue

            # Determine longs and shorts
            longs = df_sel[df_sel['gap'] > 0]
            shorts = df_sel[df_sel['gap'] < 0]

            n_long = len(longs)
            n_short = len(shorts)
            total_positions = n_long + n_short
            if total_positions == 0:
                nav_hist.append({'date': date, 'nav': nav, 'daily_return': 0.0})
                continue

            # equal-weight allocation across positions
            alloc_per_pos = nav / total_positions

            daily_pnl = 0.0
            for sec, row in longs.iterrows():
                entry = row['open'] * (1 + SLIPPAGE_PCT)  # buy at open plus slippage
                exitp = row['close'] * (1 - SLIPPAGE_PCT)
                shares = alloc_per_pos / entry
                cost = alloc_per_pos  # approximate capital used
                pnl = shares * (exitp - entry) - alloc_per_pos * TRANSACTION_COST_PCT
                daily_pnl += pnl
                trades.append({'date': date, 'sec': sec, 'side': 'long', 'entry': entry, 'exit': exitp, 'pnl': pnl})

            for sec, row in shorts.iterrows():
                entry = row['open'] * (1 - SLIPPAGE_PCT)  # sell at open minus slippage
                exitp = row['close'] * (1 + SLIPPAGE_PCT)
                shares = alloc_per_pos / entry
                pnl = shares * (entry - exitp) - alloc_per_pos * TRANSACTION_COST_PCT
                daily_pnl += pnl
                trades.append({'date': date, 'sec': sec, 'side': 'short', 'entry': entry, 'exit': exitp, 'pnl': pnl})

            daily_return = daily_pnl / nav
            nav = nav + daily_pnl
            nav_hist.append({'date': date, 'nav': nav, 'daily_return': daily_return, 'pnl': daily_pnl, 'n_positions': total_positions})
        except Exception as e:
            # never crash the loop; log and continue
            print(f"Error on {date}: {e}")
            nav_hist.append({'date': date, 'nav': nav, 'daily_return': 0.0})

    nav_df = pd.DataFrame(nav_hist).set_index('date')
    trades_df = pd.DataFrame(trades)
    return nav_df, trades_df

# -------------------------
# Run and report
# -------------------------
nav_df, trades_df = run_backtest(START, END, UNIVERSE, TOP_N)

# Simple performance printout
nav_df['cum_return'] = nav_df['nav'] / nav_df['nav'].iloc[0] - 1.0
print("Start NAV:", nav_df['nav'].iloc[0])
print("End NAV:", nav_df['nav'].iloc[-1])
print("Total return:", f"{nav_df['cum_return'].iloc[-1]*100:.2f}%")
print("Days traded:", len(nav_df))
print("Total trades:", len(trades_df))

# Plot NAV
plt.figure(figsize=(10,4))
plt.plot(pd.to_datetime(nav_df.index), nav_df['nav'])
plt.title("NAV over time")
plt.xlabel("Date")
plt.grid(True)
plt.show()

# Show sample trades
if not trades_df.empty:
    display(trades_df.sort_values('pnl', ascending=False).head(10))








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --------------------------
# INPUT: after_hours_df
# --------------------------
# You must have a DataFrame like this:
#   date        ticker          after_hours_move
#   2024-01-02  AAPL US Equity   0.025
#   2024-01-02  MSFT US Equity  -0.012
#   2024-01-03  TSLA US Equity   0.030

# Example dummy data (replace with your real universe output)
dates = pd.bdate_range("2024-01-02", "2024-01-10")
after_hours_df = pd.DataFrame({
    "date": np.repeat(dates, 3),
    "ticker": ["AAPL US Equity", "MSFT US Equity", "TSLA US Equity"] * len(dates),
    "after_hours_move": np.random.normal(0, 0.03, len(dates) * 3)
})

# --------------------------
# PARAMETERS
# --------------------------
capital = 1_000_000
slippage = 0.0005
fee = 0.0005

# --------------------------
# STEP 1: GET DAILY PRICES
# --------------------------
# In bQuant you’d replace this with BQL (PX_OPEN, PX_CLOSE).
unique_tickers = after_hours_df["ticker"].unique()
all_dates = after_hours_df["date"].unique()

price_data = {}
for t in unique_tickers:
    base = 100 + np.cumsum(np.random.randn(len(all_dates)))
    df = pd.DataFrame({
        "date": all_dates,
        "open": base * (1 + np.random.normal(0, 0.01, len(all_dates))),
        "close": base * (1 + np.random.normal(0, 0.01, len(all_dates)))
    })
    df.set_index("date", inplace=True)
    price_data[t] = df

# --------------------------
# STEP 2: BACKTEST LOOP
# --------------------------
nav = capital
nav_hist = []
trades = []

for d in all_dates:
    # Use yesterday's signals (lag)
    prev_day = pd.Timestamp(d) - pd.tseries.offsets.BDay(1)
    daily_universe = after_hours_df.query("date == @prev_day")
    if daily_universe.empty:
        nav_hist.append((d, nav))
        continue

    signals = dict(zip(
        daily_universe["ticker"],
        np.where(daily_universe["after_hours_move"] > 0, 1, -1)
    ))

    positions = list(signals.keys())
    if not positions:
        nav_hist.append((d, nav))
        continue

    weight = nav / len(positions)
    daily_pnl = 0

    for t in positions:
        if t not in price_data or d not in price_data[t].index:
            continue  # skip if missing data
        s = signals[t]
        o = price_data[t].loc[d, "open"]
        c = price_data[t].loc[d, "close"]

        entry = o * (1 + slippage * s)
        exitp = c * (1 - slippage * s)
        shares = weight / entry
        pnl = s * (exitp - entry) * shares - weight * fee
        daily_pnl += pnl

        trades.append((d, t, s, entry, exitp, pnl))

    nav += daily_pnl
    nav_hist.append((d, nav))

# --------------------------
# STEP 3: RESULTS
# --------------------------
nav_df = pd.DataFrame(nav_hist, columns=["date", "nav"]).set_index("date")

print("Start NAV:", capital)
print("End NAV:", nav_df["nav"].iloc[-1])
print("Total return: %.2f%%" % ((nav_df["nav"].iloc[-1] / capital - 1) * 100))

plt.plot(nav_df.index, nav_df["nav"])
plt.title("After-Hours Momentum (Next-Day Trading) NAV")
plt.grid(True)
plt.show()

# optional: trades dataframe
trades_df = pd.DataFrame(trades, columns=["date","ticker","signal","entry","exit","pnl"])




import pandas as pd
import numpy as np

# --------------------------
# INPUT DATA
# --------------------------
# Example after_hours_df (replace with your screen output)
dates = pd.bdate_range("2024-01-02", "2024-01-10")
after_hours_df = pd.DataFrame({
    "date": np.repeat(dates, 3),
    "ticker": ["AAPL", "MSFT", "TSLA"] * len(dates),
    "after_hours_move": np.random.normal(0, 0.03, len(dates)*3)
})

# Fake open/close prices (replace with real BQL data)
prices = []
for t in after_hours_df["ticker"].unique():
    base = 100 + np.cumsum(np.random.randn(len(dates)))
    df = pd.DataFrame({
        "date": dates,
        "ticker": t,
        "open": base * (1 + np.random.normal(0,0.01,len(dates))),
        "close": base * (1 + np.random.normal(0,0.01,len(dates)))
    })
    prices.append(df)
prices_df = pd.concat(prices)

# --------------------------
# BACKTEST
# --------------------------
capital = 1_000_000
nav = capital
nav_hist = []

for d in dates[1:]:  # start from 2nd day (need yesterday's signals)
    # yesterday's movers
    prev = after_hours_df[after_hours_df["date"] == d - pd.tseries.offsets.BDay(1)]
    if prev.empty:
        nav_hist.append((d, nav))
        continue

    # signals
    prev["signal"] = np.where(prev["after_hours_move"] > 0, 1, -1)

    # today’s prices
    today = prices_df[prices_df["date"] == d]
    df = prev.merge(today, on="ticker", how="inner")

    if df.empty:
        nav_hist.append((d, nav))
        continue

    weight = nav / len(df)
    df["entry"] = df["open"]
    df["exit"] = df["close"]
    df["pnl"] = df["signal"] * (df["exit"] - df["entry"]) * (weight / df["entry"])

    nav += df["pnl"].sum()
    nav_hist.append((d, nav))

# --------------------------
# RESULTS
# --------------------------
nav_df = pd.DataFrame(nav_hist, columns=["date","nav"]).set_index("date")
print("Start NAV:", capital)
print("End NAV:", nav_df.iloc[-1,0])
print("Return %.2f%%" % ((nav_df.iloc[-1,0]/capital - 1)*100))

nav_df.plot(title="After Hours Momentum NAV", grid=True)
